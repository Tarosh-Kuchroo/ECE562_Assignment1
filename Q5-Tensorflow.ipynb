{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6tz16hrUCN2"
   },
   "outputs": [],
   "source": [
    "# # this mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# # enter the foldername in your Drive where you have saved the unzipped\n",
    "# # assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
    "# FOLDERNAME = None\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# # symlink to make it easier to load your files\n",
    "# !ln -s \"/content/drive/My Drive/$FOLDERNAME\" \"/content/assignment2\"\n",
    "\n",
    "# # now that we've mounted your Drive, this ensures that\n",
    "# # the Python interpreter of the Colab VM can load\n",
    "# # python files from within it.\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "\n",
    "# %cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wp6QB_7eUCN5",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you choose to work with that notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8LFEls7UCN5",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRWUkCTYUCN6",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here.\n",
    "\n",
    "**NOTE: This notebook is meant to teach you the latest version of Tensorflow which is as of this homework version `2.2.0-rc3`. Most examples on the web today are still in 1.x, so be careful not to confuse the two when looking up documentation**.\n",
    "\n",
    "## Install Tensorflow 2.0 (ONLY IF YOU ARE WORKING LOCALLY)\n",
    "\n",
    "1. Have the latest version of Anaconda installed on your machine.\n",
    "2. Create a new conda environment starting from Python 3.7. In this setup example, we'll call it `tf_20_env`.\n",
    "3. Run the command: `source activate tf_20_env`\n",
    "4. Then pip install TF 2.0 as described here: https://www.tensorflow.org/install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4INjo7aUCN6",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "This notebook has 5 parts. We will walk through TensorFlow at **three different levels of abstraction**, which should help you better understand it and prepare you for working on your project.\n",
    "\n",
    "1. Part I, Preparation: load the CINIC-10 dataset.\n",
    "2. Part II, Barebone TensorFlow: **Abstraction Level 1**, we will work directly with low-level TensorFlow graphs. \n",
    "3. Part III, Keras Model API: **Abstraction Level 2**, we will use `tf.keras.Model` to define arbitrary neural network architecture. \n",
    "4. Part IV, Keras Sequential + Functional API: **Abstraction Level 3**, we will use `tf.keras.Sequential` to define a linear feed-forward network very conveniently, and then explore the functional libraries for building unique and uncommon models that require more flexibility.\n",
    "5. Part V, CINIC-10 open-ended challenge: please implement your own network to get as high accuracy as possible on CINIC-10. You can experiment with any layer, optimizer, hyperparameters or other advanced features. \n",
    "\n",
    "We will discuss Keras in more detail later in the notebook.\n",
    "\n",
    "Here is a table of comparison:\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `tf.keras.Model`     | High        | Medium      |\n",
    "| `tf.keras.Sequential` | Low         | High        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd5V9yUcUCN7"
   },
   "source": [
    "# Part I: Preparation\n",
    "\n",
    "First, we load the CINIC-10 dataset. This might take a few minutes to download the first time you run it, but after that the files should be cached on disk and loading should be faster.\n",
    "\n",
    "For the purposes of this assignment we will still write our own code to preprocess the data and iterate through it in minibatches. The `tf.data` package in TensorFlow provides tools for automating this process, but working with this package adds extra complication and is beyond the scope of this notebook. However using `tf.data` can be much more efficient than the simple approach used in this notebook, so you should consider using it for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ggQ5mmpIUCN7",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from ece697ls.data_utils import get_CINIC10_data\n",
    "from ece697ls.pruning_helper import invert_ch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "cIfw7QxtUCN9",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (53973, 3, 32, 32)\n",
      "Train labels shape:  (53973,) int32\n",
      "Validation data shape:  (10195, 3, 32, 32)\n",
      "Validation labels shape:  (10195,)\n",
      "Test data shape:  (10196, 3, 32, 32)\n",
      "Test labels shape:  (10196,)\n"
     ]
    }
   ],
   "source": [
    "def load_cinic10():\n",
    "  data = get_CINIC10_data()\n",
    "\n",
    "  # Convert to float32 for TensorFlow\n",
    "  X_train = data['X_train'].astype(np.float32)\n",
    "  X_val = data['X_val'].astype(np.float32)\n",
    "  X_test = data['X_test'].astype(np.float32)\n",
    "\n",
    "  y_train = data['y_train'].astype(np.int32)\n",
    "  y_val = data['y_val'].astype(np.int32)\n",
    "  y_test = data['y_test'].astype(np.int32)\n",
    "\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "data = get_CINIC10_data()\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cinic10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "qn3f9z6gUCN_",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "WFbG6VcjUCOB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 3, 32, 32) (64,)\n",
      "1 (64, 3, 32, 32) (64,)\n",
      "2 (64, 3, 32, 32) (64,)\n",
      "3 (64, 3, 32, 32) (64,)\n",
      "4 (64, 3, 32, 32) (64,)\n",
      "5 (64, 3, 32, 32) (64,)\n",
      "6 (64, 3, 32, 32) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ufSGTmRUCOD"
   },
   "source": [
    "You can optionally **use GPU by setting the flag to True below**.\n",
    "\n",
    "## Colab Users\n",
    "\n",
    "If you are using Colab, you need to manually switch to a GPU device. You can do this by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. Note that you have to rerun the cells from the top since the kernel gets restarted upon switching runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNTsQHGiUCOD",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# # Set up some global variables\n",
    "# USE_GPU = True\n",
    "\n",
    "# if USE_GPU:\n",
    "#     device = '/device:GPU:0'\n",
    "# else:\n",
    "#     device = '/cpu:0'\n",
    "\n",
    "# # Constant to control how often we print when training models\n",
    "# print_every = 100\n",
    "\n",
    "# print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Fjrfy-UCOF",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Part II: Barebones TensorFlow\n",
    "TensorFlow ships with various high-level APIs which make it very convenient to define and train neural networks; we will cover some of these constructs in Part III and Part IV of this notebook. In this section we will start by building a model with basic TensorFlow constructs to help you better understand what's going on under the hood of the higher-level APIs.\n",
    "\n",
    "**\"Barebones Tensorflow\" is important to understanding the building blocks of TensorFlow, but much of it involves concepts from TensorFlow 1.x.** We will be working with legacy modules such as `tf.Variable`.\n",
    "\n",
    "Therefore, please read and understand the differences between legacy (1.x) TF and the new (2.0) TF.\n",
    "\n",
    "### Historical background on TensorFlow 1.x\n",
    "\n",
    "TensorFlow 1.x is primarily a framework for working with **static computational graphs**. Nodes in the computational graph are Tensors which will hold n-dimensional arrays when the graph is run; edges in the graph represent functions that will operate on Tensors when the graph is run to actually perform useful computation.\n",
    "\n",
    "Before Tensorflow 2.0, we had to configure the graph into two phases. There are plenty of tutorials online that explain this two-step process. The process generally looks like the following for TF 1.x:\n",
    "1. **Build a computational graph that describes the computation that you want to perform**. This stage doesn't actually perform any computation; it just builds up a symbolic representation of your computation. This stage will typically define one or more `placeholder` objects that represent inputs to the computational graph.\n",
    "2. **Run the computational graph many times.** Each time the graph is run (e.g. for one gradient descent step) you will specify which parts of the graph you want to compute, and pass a `feed_dict` dictionary that will give concrete values to any `placeholder`s in the graph.\n",
    "\n",
    "### The new paradigm in Tensorflow 2.0\n",
    "Now, with Tensorflow 2.0, we can simply adopt a functional form that is more Pythonic and similar in spirit to PyTorch and direct Numpy operation. Instead of the 2-step paradigm with computation graphs, making it (among other things) easier to debug TF code. You can read more details at https://www.tensorflow.org/guide/eager.\n",
    "\n",
    "The main difference between the TF 1.x and 2.0 approach is that the 2.0 approach doesn't make use of `tf.Session`, `tf.run`, `placeholder`, `feed_dict`. To get more details of what's different between the two version and how to convert between the two, check out the official migration guide: https://www.tensorflow.org/alpha/guide/migration_guide\n",
    "\n",
    "Later, in the rest of this notebook we'll focus on this new, simpler approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0xZUupsUCOG",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### TensorFlow warmup: Flatten Function\n",
    "\n",
    "We can see this in action by defining a simple `flatten` function that will reshape image data for use in a fully-connected network.\n",
    "\n",
    "In TensorFlow, data for convolutional feature maps is typically stored in a Tensor of shape N x H x W x C where:\n",
    "\n",
    "- N is the number of datapoints (minibatch size)\n",
    "- H is the height of the feature map\n",
    "- W is the width of the feature map\n",
    "- C is the number of channels in the feature map\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we use fully connected affine layers to process the image, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"flatten\" operation to collapse the `H x W x C` values per representation into a single long vector. \n",
    "\n",
    "Notice the `tf.reshape` call has the target shape as `(N, -1)`, meaning it will reshape/keep the first dimension to be N, and then infer as necessary what the second dimension is in the output, so we can collapse the remaining dimensions from the input properly.\n",
    "\n",
    "**NOTE**: TensorFlow and PyTorch differ on the default Tensor layout; TensorFlow uses N x H x W x C but PyTorch uses N x C x H x W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "kXbWGC_fUCOG",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "    \n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "uvkBgaIcUCOI",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_np:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]] \n",
      "\n",
      "x_flat_np:\n",
      " tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17 18 19 20 21 22 23]], shape=(2, 12), dtype=int64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_flatten():\n",
    "    # Construct concrete values of the input data x using numpy\n",
    "    x_np = np.arange(24).reshape((2, 3, 4))\n",
    "    print('x_np:\\n', x_np, '\\n')\n",
    "    # Compute a concrete output value.\n",
    "    x_flat_np = flatten(x_np)\n",
    "    print('x_flat_np:\\n', x_flat_np, '\\n')\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mNTDtapUCOJ"
   },
   "source": [
    "### Barebones TensorFlow: Define a Two-Layer Network\n",
    "We will now implement our first neural network with TensorFlow: a fully-connected ReLU network with two hidden layers and no biases on the CINIC10 dataset. For now we will use only low-level TensorFlow operators to define the network; later we will see how to use the higher-level abstractions provided by `tf.keras` to simplify the process.\n",
    "\n",
    "We will define the forward pass of the network in the function `two_layer_fc`; this will accept TensorFlow Tensors for the inputs and weights of the network, and return a TensorFlow Tensor for the scores. \n",
    "\n",
    "After defining the network architecture in the `two_layer_fc` function, we will test the implementation by checking the shape of the output.\n",
    "\n",
    "**It's important that you read and understand this implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "BZR7RldLUCOK",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network; the architecture is:\n",
    "    fully-connected layer -> ReLU -> fully connected layer.\n",
    "    Note that we only need to define the forward pass here; TensorFlow will take\n",
    "    care of computing the gradients for us.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the\n",
    "      network, where w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores\n",
    "      for the input data x.\n",
    "    \"\"\"\n",
    "    w1, w2 = params                   # Unpack the parameters\n",
    "    x = flatten(x)                    # Flatten the input; now x has shape (N, D)\n",
    "    h = tf.nn.relu(tf.matmul(x, w1))  # Hidden layer: h has shape (N, H)\n",
    "    scores = tf.matmul(h, w2)         # Compute scores of shape (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "I4YfpK6uUCOL",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 6)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_test():\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Scoping our TF operations under a tf.device context manager \n",
    "    # lets us tell TensorFlow where we want these Tensors to be\n",
    "    # multiplied and/or operated on, e.g. on a CPU or a GPU.\n",
    "    device = \"/CPU:0\"   # using my CPU\n",
    "\n",
    "\n",
    "    with tf.device(device):        \n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))\n",
    "        w2 = tf.zeros((hidden_layer_size, 6))\n",
    "\n",
    "        # Call our two_layer_fc function for the forward pass of the network.\n",
    "        scores = two_layer_fc(x, [w1, w2])\n",
    "\n",
    "    print(scores.shape)\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZiC-4JLUCOR"
   },
   "source": [
    "### Barebones TensorFlow: Three-Layer ConvNet\n",
    "Here you will complete the implementation of the function `three_layer_convnet` which will perform the forward pass of a three-layer convolutional network. The network should have the following architecture:\n",
    "\n",
    "1. A convolutional layer (with bias) with `channel_1` filters, each with shape `KW1 x KH1`, and zero-padding of two\n",
    "2. ReLU nonlinearity\n",
    "3. A convolutional layer (with bias) with `channel_2` filters, each with shape `KW2 x KH2`, and zero-padding of one\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer with bias, producing scores for `C` classes.\n",
    "\n",
    "**HINT**: For convolutions: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/conv2d; be careful with padding!\n",
    "\n",
    "**HINT**: For biases: https://www.tensorflow.org/performance/xla/broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Gq6XHKwNUCOR"
   },
   "outputs": [],
   "source": [
    "device = \"/CPU:0\"  # using my CPU\n",
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the architecture described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images\n",
    "    - params: A list of TensorFlow Tensors giving the weights and biases for the\n",
    "      network; should contain the following:\n",
    "      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving\n",
    "        weights for the first convolutional layer.\n",
    "      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the\n",
    "        first convolutional layer.\n",
    "      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)\n",
    "        giving weights for the second convolutional layer\n",
    "      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the\n",
    "        second convolutional layer.\n",
    "      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    x = tf.transpose(x, perm=[0, 2, 3, 1])\n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the three-layer ConvNet.            #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    x_padded = tf.pad(x, [[0, 0], [2, 2], [2, 2], [0, 0]]) # added the pad height and width dimensions by 2 on each side\n",
    "    conv1 = tf.nn.conv2d(x_padded, conv_w1, strides=[1, 1, 1, 1], padding='VALID') # 'VALID' padding since we've already padded the input\n",
    "    conv1 = tf.nn.bias_add(conv1, conv_b1) # Add the bias term to the convolution output\n",
    "    relu1 = tf.nn.relu(conv1) # ReLU activation for non-linearity\n",
    "\n",
    "    relu1_padded = tf.pad(relu1, [[0, 0], [1, 1], [1, 1], [0, 0]]) # Pad height and width dimensions by 1 on each side\n",
    "    conv2 = tf.nn.conv2d(relu1_padded, conv_w2, strides=[1, 1, 1, 1], padding='VALID') # 'VALID' padding since we've already padded the input\n",
    "    conv2 = tf.nn.bias_add(conv2, conv_b2) # Add the bias term to the convolution output\n",
    "    relu2 = tf.nn.relu(conv2) # ReLU activation again for non-linearity\n",
    "\n",
    "    flat = tf.reshape(relu2, [tf.shape(relu2)[0], -1]) # Flatten the output to shape (N, D)\n",
    "    scores = tf.matmul(flat, fc_w) + fc_b # Fully connected layer to get scores of shape (N, C)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                              END OF YOUR CODE                            #\n",
    "    ############################################################################\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7mt1iLEUCOT"
   },
   "source": [
    "After defing the forward pass of the three-layer ConvNet above, run the following cell to test your implementation. Like the two-layer network, we run the graph on a batch of zeros just to make sure the function doesn't crash, and produces outputs of the correct shape.\n",
    "\n",
    "When you run this function, `scores_np` should have shape `(64, 6)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "barebones_output_shape",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_np has shape:  (64, 6)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "    \n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        conv_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv_b1 = tf.zeros((6,))\n",
    "        conv_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv_b2 = tf.zeros((9,))\n",
    "        fc_w = tf.zeros((32 * 32 * 9, 6))\n",
    "        fc_b = tf.zeros((6,))\n",
    "        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "        scores = three_layer_convnet(x, params)\n",
    "\n",
    "    # Inputs to convolutional layers are 4-dimensional arrays with shape\n",
    "    # [batch_size, height, width, channels]\n",
    "    print('scores_np has shape: ', scores.shape)\n",
    "\n",
    "three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GD4PNi0UCOV"
   },
   "source": [
    "### Barebones TensorFlow: Training Step\n",
    "\n",
    "We now define the `training_step` function performs a single training step. This will take three basic steps:\n",
    "\n",
    "1. Compute the loss\n",
    "2. Compute the gradient of the loss with respect to all network weights\n",
    "3. Make a weight update step using (stochastic) gradient descent.\n",
    "\n",
    "\n",
    "We need to use a few new TensorFlow functions to do all of this:\n",
    "- For computing the cross-entropy loss we'll use `tf.nn.sparse_softmax_cross_entropy_with_logits`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "- For averaging the loss across a minibatch of data we'll use `tf.reduce_mean`:\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/reduce_mean\n",
    "\n",
    "- For computing gradients of the loss with respect to the weights we'll use `tf.GradientTape` (useful for Eager execution):  https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape\n",
    "\n",
    "- We'll mutate the weight values stored in a TensorFlow Tensor using `tf.assign_sub` (\"sub\" is for subtraction): https://www.tensorflow.org/api_docs/python/tf/assign_sub \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "yc_OSV9pUCOV",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def training_step(model_fn, x, y, params, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        scores = model_fn(x, params) # Forward pass of the model\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        grad_params = tape.gradient(total_loss, params)\n",
    "\n",
    "        # Make a vanilla gradient descent step on all of the model parameters\n",
    "        # Manually update the weights using assign_sub()\n",
    "        for w, grad_w in zip(params, grad_params):\n",
    "            w.assign_sub(learning_rate * grad_w)\n",
    "                        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "zSyEnu6QUCOX",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "num_iters = 200 # number of iterations to run not added before\n",
    "batch_size = 50 # batch size not added before alos not added before\n",
    "print_every = 10 # how often to print not added before\n",
    "device = \"/CPU:0\"\n",
    "def train_part2(model_fn, init_fn, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CINIC-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model\n",
    "      using TensorFlow; it should have the following signature:\n",
    "      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n",
    "      minibatch of image data, params is a list of TensorFlow Tensors holding\n",
    "      the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n",
    "      giving scores for all elements of x.\n",
    "    - init_fn: A Python function that initializes the parameters of the model.\n",
    "      It should have the signature params = init_fn() where params is a list\n",
    "      of TensorFlow Tensors holding the (randomly initialized) weights of the\n",
    "      model.\n",
    "    - learning_rate: Python float giving the learning rate to use for SGD.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    params = init_fn()  # Initialize the model parameters            \n",
    "        \n",
    "    for t, (x_np, y_np) in enumerate(train_dset):\n",
    "        # Run the graph on a batch of training data.\n",
    "        loss = training_step(model_fn, x_np, y_np, params, learning_rate)\n",
    "        \n",
    "        # Periodically print the loss and check accuracy on the val set.\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "            check_accuracy(val_dset, x_np, model_fn, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "KZh_Wk6lUCOZ",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def check_accuracy(dset, x, model_fn, params):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model, e.g. for validation.\n",
    "    \n",
    "    Inputs:\n",
    "    - dset: A Dataset object against which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - model_fn: the Model we will be calling to make predictions on x\n",
    "    - params: parameters for the model_fn to work with\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        scores_np = model_fn(x_batch, params).numpy()\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfNV7mfsUCOb"
   },
   "source": [
    "### Barebones TensorFlow: Initialization\n",
    "We'll use the following utility method to initialize the weight matrices for our models using Kaiming's normalization method.\n",
    "\n",
    "[1] He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "7VWXO-TbUCOb"
   },
   "outputs": [],
   "source": [
    "def create_matrix_with_kaiming_normal(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "    return tf.keras.backend.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nopr5J4RUCOd"
   },
   "source": [
    "### Barebones TensorFlow: Train a Two-Layer Network\n",
    "We are finally ready to use all of the pieces defined above to train a two-layer fully-connected network on CINIC-10.\n",
    "\n",
    "We just need to define a function to initialize the weights of the model, and call `train_part2`.\n",
    "\n",
    "Defining the weights of the network introduces another important piece of TensorFlow API: `tf.Variable`. A TensorFlow Variable is a Tensor whose value is stored in the graph and persists across runs of the computational graph; however unlike constants defined with `tf.zeros` or `tf.random_normal`, the values of a Variable can be mutated as the graph runs; these mutations will persist across graph runs. Learnable parameters of the network are usually stored in Variables.\n",
    "\n",
    "You don't need to tune any hyperparameters, but you should achieve validation accuracies above 30% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "5jq_1Sk6UCOd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 111.0117\n",
      "Got 1910 / 10195 correct (18.73%)\n",
      "Iteration 10, loss = 69.9516\n",
      "Got 2063 / 10195 correct (20.24%)\n",
      "Iteration 20, loss = 80.0376\n",
      "Got 2473 / 10195 correct (24.26%)\n",
      "Iteration 30, loss = 83.1900\n",
      "Got 2511 / 10195 correct (24.63%)\n",
      "Iteration 40, loss = 76.4602\n",
      "Got 2448 / 10195 correct (24.01%)\n",
      "Iteration 50, loss = 67.1428\n",
      "Got 2567 / 10195 correct (25.18%)\n",
      "Iteration 60, loss = 87.6555\n",
      "Got 2647 / 10195 correct (25.96%)\n",
      "Iteration 70, loss = 68.7226\n",
      "Got 2647 / 10195 correct (25.96%)\n",
      "Iteration 80, loss = 43.5270\n",
      "Got 2628 / 10195 correct (25.78%)\n",
      "Iteration 90, loss = 67.0045\n",
      "Got 2556 / 10195 correct (25.07%)\n",
      "Iteration 100, loss = 96.2568\n",
      "Got 2448 / 10195 correct (24.01%)\n",
      "Iteration 110, loss = 57.7961\n",
      "Got 2671 / 10195 correct (26.20%)\n",
      "Iteration 120, loss = 65.4543\n",
      "Got 2931 / 10195 correct (28.75%)\n",
      "Iteration 130, loss = 79.3598\n",
      "Got 2660 / 10195 correct (26.09%)\n",
      "Iteration 140, loss = 55.9113\n",
      "Got 2910 / 10195 correct (28.54%)\n",
      "Iteration 150, loss = 68.9272\n",
      "Got 2501 / 10195 correct (24.53%)\n",
      "Iteration 160, loss = 63.8716\n",
      "Got 2729 / 10195 correct (26.77%)\n",
      "Iteration 170, loss = 75.6322\n",
      "Got 2395 / 10195 correct (23.49%)\n",
      "Iteration 180, loss = 57.3584\n",
      "Got 2615 / 10195 correct (25.65%)\n",
      "Iteration 190, loss = 83.4646\n",
      "Got 2713 / 10195 correct (26.61%)\n",
      "Iteration 200, loss = 59.7476\n",
      "Got 2830 / 10195 correct (27.76%)\n",
      "Iteration 210, loss = 67.0222\n",
      "Got 2806 / 10195 correct (27.52%)\n",
      "Iteration 220, loss = 69.6147\n",
      "Got 2952 / 10195 correct (28.96%)\n",
      "Iteration 230, loss = 76.7767\n",
      "Got 2885 / 10195 correct (28.30%)\n",
      "Iteration 240, loss = 45.7434\n",
      "Got 2427 / 10195 correct (23.81%)\n",
      "Iteration 250, loss = 77.7974\n",
      "Got 2448 / 10195 correct (24.01%)\n",
      "Iteration 260, loss = 79.0243\n",
      "Got 3084 / 10195 correct (30.25%)\n",
      "Iteration 270, loss = 58.9802\n",
      "Got 3152 / 10195 correct (30.92%)\n",
      "Iteration 280, loss = 57.4385\n",
      "Got 2923 / 10195 correct (28.67%)\n",
      "Iteration 290, loss = 45.1638\n",
      "Got 3001 / 10195 correct (29.44%)\n",
      "Iteration 300, loss = 67.6472\n",
      "Got 2529 / 10195 correct (24.81%)\n",
      "Iteration 310, loss = 63.0009\n",
      "Got 2411 / 10195 correct (23.65%)\n",
      "Iteration 320, loss = 57.7408\n",
      "Got 2719 / 10195 correct (26.67%)\n",
      "Iteration 330, loss = 66.9257\n",
      "Got 2944 / 10195 correct (28.88%)\n",
      "Iteration 340, loss = 69.8204\n",
      "Got 3171 / 10195 correct (31.10%)\n",
      "Iteration 350, loss = 46.3067\n",
      "Got 3103 / 10195 correct (30.44%)\n",
      "Iteration 360, loss = 71.8657\n",
      "Got 3104 / 10195 correct (30.45%)\n",
      "Iteration 370, loss = 59.7810\n",
      "Got 2771 / 10195 correct (27.18%)\n",
      "Iteration 380, loss = 66.2156\n",
      "Got 2380 / 10195 correct (23.34%)\n",
      "Iteration 390, loss = 48.3212\n",
      "Got 3204 / 10195 correct (31.43%)\n",
      "Iteration 400, loss = 55.3552\n",
      "Got 2840 / 10195 correct (27.86%)\n",
      "Iteration 410, loss = 103.8850\n",
      "Got 2454 / 10195 correct (24.07%)\n",
      "Iteration 420, loss = 35.7841\n",
      "Got 2506 / 10195 correct (24.58%)\n",
      "Iteration 430, loss = 62.9834\n",
      "Got 2826 / 10195 correct (27.72%)\n",
      "Iteration 440, loss = 51.0530\n",
      "Got 2945 / 10195 correct (28.89%)\n",
      "Iteration 450, loss = 54.0894\n",
      "Got 2873 / 10195 correct (28.18%)\n",
      "Iteration 460, loss = 66.3844\n",
      "Got 2898 / 10195 correct (28.43%)\n",
      "Iteration 470, loss = 51.5245\n",
      "Got 3219 / 10195 correct (31.57%)\n",
      "Iteration 480, loss = 46.9058\n",
      "Got 3160 / 10195 correct (31.00%)\n",
      "Iteration 490, loss = 40.0851\n",
      "Got 2991 / 10195 correct (29.34%)\n",
      "Iteration 500, loss = 73.1593\n",
      "Got 2820 / 10195 correct (27.66%)\n",
      "Iteration 510, loss = 44.8836\n",
      "Got 2792 / 10195 correct (27.39%)\n",
      "Iteration 520, loss = 57.3933\n",
      "Got 3083 / 10195 correct (30.24%)\n",
      "Iteration 530, loss = 62.6266\n",
      "Got 3041 / 10195 correct (29.83%)\n",
      "Iteration 540, loss = 68.3653\n",
      "Got 3360 / 10195 correct (32.96%)\n",
      "Iteration 550, loss = 61.8665\n",
      "Got 2751 / 10195 correct (26.98%)\n",
      "Iteration 560, loss = 54.7149\n",
      "Got 3104 / 10195 correct (30.45%)\n",
      "Iteration 570, loss = 51.6352\n",
      "Got 2836 / 10195 correct (27.82%)\n",
      "Iteration 580, loss = 52.8550\n",
      "Got 3236 / 10195 correct (31.74%)\n",
      "Iteration 590, loss = 52.0117\n",
      "Got 2976 / 10195 correct (29.19%)\n",
      "Iteration 600, loss = 48.2409\n",
      "Got 3253 / 10195 correct (31.91%)\n",
      "Iteration 610, loss = 53.5039\n",
      "Got 2840 / 10195 correct (27.86%)\n",
      "Iteration 620, loss = 42.9020\n",
      "Got 3188 / 10195 correct (31.27%)\n",
      "Iteration 630, loss = 52.8240\n",
      "Got 3321 / 10195 correct (32.57%)\n",
      "Iteration 640, loss = 47.9426\n",
      "Got 3050 / 10195 correct (29.92%)\n",
      "Iteration 650, loss = 54.8136\n",
      "Got 2913 / 10195 correct (28.57%)\n",
      "Iteration 660, loss = 43.4961\n",
      "Got 2821 / 10195 correct (27.67%)\n",
      "Iteration 670, loss = 38.6970\n",
      "Got 3160 / 10195 correct (31.00%)\n",
      "Iteration 680, loss = 39.9983\n",
      "Got 3314 / 10195 correct (32.51%)\n",
      "Iteration 690, loss = 32.2980\n",
      "Got 3407 / 10195 correct (33.42%)\n",
      "Iteration 700, loss = 50.6284\n",
      "Got 3258 / 10195 correct (31.96%)\n",
      "Iteration 710, loss = 49.3588\n",
      "Got 2841 / 10195 correct (27.87%)\n",
      "Iteration 720, loss = 43.9415\n",
      "Got 2621 / 10195 correct (25.71%)\n",
      "Iteration 730, loss = 49.7362\n",
      "Got 3057 / 10195 correct (29.99%)\n",
      "Iteration 740, loss = 55.5518\n",
      "Got 3041 / 10195 correct (29.83%)\n",
      "Iteration 750, loss = 47.9970\n",
      "Got 2955 / 10195 correct (28.98%)\n",
      "Iteration 760, loss = 45.6526\n",
      "Got 3098 / 10195 correct (30.39%)\n",
      "Iteration 770, loss = 44.1752\n",
      "Got 2949 / 10195 correct (28.93%)\n",
      "Iteration 780, loss = 44.5563\n",
      "Got 3039 / 10195 correct (29.81%)\n",
      "Iteration 790, loss = 51.0979\n",
      "Got 3071 / 10195 correct (30.12%)\n",
      "Iteration 800, loss = 68.4401\n",
      "Got 3402 / 10195 correct (33.37%)\n",
      "Iteration 810, loss = 27.7724\n",
      "Got 3353 / 10195 correct (32.89%)\n",
      "Iteration 820, loss = 39.0528\n",
      "Got 3195 / 10195 correct (31.34%)\n",
      "Iteration 830, loss = 45.6809\n",
      "Got 2735 / 10195 correct (26.83%)\n",
      "Iteration 840, loss = 55.4847\n",
      "Got 2754 / 10195 correct (27.01%)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a two-layer network, for use with the\n",
    "    two_layer_network function defined above. \n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow tf.Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow tf.Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(create_matrix_with_kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(create_matrix_with_kaiming_normal((4000, 6)))\n",
    "    return [w1, w2]\n",
    "\n",
    "learning_rate = 1e-4\n",
    "train_part2(two_layer_fc, two_layer_fc_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdu6j8kEUCOf"
   },
   "source": [
    "### Barebones TensorFlow: Train a three-layer ConvNet\n",
    "We will now use TensorFlow to train a three-layer ConvNet on CINIC-10.\n",
    "\n",
    "You need to implement the `three_layer_convnet_init` function. Recall that the architecture of the network is:\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer (with bias) to compute scores for 6 classes\n",
    "\n",
    "You don't need to do any hyperparameter tuning, but you should see validation accuracies above 30% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "barebones_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 157.6129\n",
      "Got 1712 / 10195 correct (16.79%)\n",
      "Iteration 10, loss = 17.2059\n",
      "Got 2341 / 10195 correct (22.96%)\n",
      "Iteration 20, loss = 15.1101\n",
      "Got 2378 / 10195 correct (23.33%)\n",
      "Iteration 30, loss = 13.7050\n",
      "Got 2363 / 10195 correct (23.18%)\n",
      "Iteration 40, loss = 11.8909\n",
      "Got 2392 / 10195 correct (23.46%)\n",
      "Iteration 50, loss = 11.6599\n",
      "Got 2366 / 10195 correct (23.21%)\n",
      "Iteration 60, loss = 9.1301\n",
      "Got 2306 / 10195 correct (22.62%)\n",
      "Iteration 70, loss = 7.8653\n",
      "Got 2230 / 10195 correct (21.87%)\n",
      "Iteration 80, loss = 6.9172\n",
      "Got 2238 / 10195 correct (21.95%)\n",
      "Iteration 90, loss = 7.2950\n",
      "Got 2265 / 10195 correct (22.22%)\n",
      "Iteration 100, loss = 5.7653\n",
      "Got 2225 / 10195 correct (21.82%)\n",
      "Iteration 110, loss = 4.5083\n",
      "Got 2200 / 10195 correct (21.58%)\n",
      "Iteration 120, loss = 5.0539\n",
      "Got 2228 / 10195 correct (21.85%)\n",
      "Iteration 130, loss = 5.0198\n",
      "Got 2258 / 10195 correct (22.15%)\n",
      "Iteration 140, loss = 5.0178\n",
      "Got 2253 / 10195 correct (22.10%)\n",
      "Iteration 150, loss = 4.9254\n",
      "Got 2233 / 10195 correct (21.90%)\n",
      "Iteration 160, loss = 4.3191\n",
      "Got 2239 / 10195 correct (21.96%)\n",
      "Iteration 170, loss = 3.8063\n",
      "Got 2231 / 10195 correct (21.88%)\n",
      "Iteration 180, loss = 3.1781\n",
      "Got 2219 / 10195 correct (21.77%)\n",
      "Iteration 190, loss = 3.8092\n",
      "Got 2180 / 10195 correct (21.38%)\n",
      "Iteration 200, loss = 3.2833\n",
      "Got 2196 / 10195 correct (21.54%)\n",
      "Iteration 210, loss = 3.6496\n",
      "Got 2169 / 10195 correct (21.28%)\n",
      "Iteration 220, loss = 2.8057\n",
      "Got 2146 / 10195 correct (21.05%)\n",
      "Iteration 230, loss = 3.2745\n",
      "Got 2151 / 10195 correct (21.10%)\n",
      "Iteration 240, loss = 3.0397\n",
      "Got 2168 / 10195 correct (21.27%)\n",
      "Iteration 250, loss = 3.2623\n",
      "Got 2144 / 10195 correct (21.03%)\n",
      "Iteration 260, loss = 2.5540\n",
      "Got 2140 / 10195 correct (20.99%)\n",
      "Iteration 270, loss = 2.8992\n",
      "Got 2128 / 10195 correct (20.87%)\n",
      "Iteration 280, loss = 2.7792\n",
      "Got 2135 / 10195 correct (20.94%)\n",
      "Iteration 290, loss = 2.7764\n",
      "Got 2128 / 10195 correct (20.87%)\n",
      "Iteration 300, loss = 2.8326\n",
      "Got 2110 / 10195 correct (20.70%)\n",
      "Iteration 310, loss = 3.0848\n",
      "Got 2105 / 10195 correct (20.65%)\n",
      "Iteration 320, loss = 2.6959\n",
      "Got 2101 / 10195 correct (20.61%)\n",
      "Iteration 330, loss = 2.6111\n",
      "Got 2107 / 10195 correct (20.67%)\n",
      "Iteration 340, loss = 2.3758\n",
      "Got 2081 / 10195 correct (20.41%)\n",
      "Iteration 350, loss = 2.0965\n",
      "Got 2100 / 10195 correct (20.60%)\n",
      "Iteration 360, loss = 2.7552\n",
      "Got 2112 / 10195 correct (20.72%)\n",
      "Iteration 370, loss = 2.7009\n",
      "Got 2105 / 10195 correct (20.65%)\n",
      "Iteration 380, loss = 2.1877\n",
      "Got 2110 / 10195 correct (20.70%)\n",
      "Iteration 390, loss = 2.3472\n",
      "Got 2109 / 10195 correct (20.69%)\n",
      "Iteration 400, loss = 2.3887\n",
      "Got 2118 / 10195 correct (20.77%)\n",
      "Iteration 410, loss = 2.3908\n",
      "Got 2120 / 10195 correct (20.79%)\n",
      "Iteration 420, loss = 2.2904\n",
      "Got 2143 / 10195 correct (21.02%)\n",
      "Iteration 430, loss = 2.1709\n",
      "Got 2157 / 10195 correct (21.16%)\n",
      "Iteration 440, loss = 2.2065\n",
      "Got 2151 / 10195 correct (21.10%)\n",
      "Iteration 450, loss = 2.2533\n",
      "Got 2155 / 10195 correct (21.14%)\n",
      "Iteration 460, loss = 2.3353\n",
      "Got 2161 / 10195 correct (21.20%)\n",
      "Iteration 470, loss = 2.0693\n",
      "Got 2143 / 10195 correct (21.02%)\n",
      "Iteration 480, loss = 2.1974\n",
      "Got 2127 / 10195 correct (20.86%)\n",
      "Iteration 490, loss = 1.8943\n",
      "Got 2116 / 10195 correct (20.76%)\n",
      "Iteration 500, loss = 2.0711\n",
      "Got 2109 / 10195 correct (20.69%)\n",
      "Iteration 510, loss = 2.2605\n",
      "Got 2118 / 10195 correct (20.77%)\n",
      "Iteration 520, loss = 2.2709\n",
      "Got 2129 / 10195 correct (20.88%)\n",
      "Iteration 530, loss = 2.0725\n",
      "Got 2125 / 10195 correct (20.84%)\n",
      "Iteration 540, loss = 2.1034\n",
      "Got 2119 / 10195 correct (20.78%)\n",
      "Iteration 550, loss = 1.8326\n",
      "Got 2117 / 10195 correct (20.77%)\n",
      "Iteration 560, loss = 2.1759\n",
      "Got 2120 / 10195 correct (20.79%)\n",
      "Iteration 570, loss = 2.0972\n",
      "Got 2111 / 10195 correct (20.71%)\n",
      "Iteration 580, loss = 2.2285\n",
      "Got 2124 / 10195 correct (20.83%)\n",
      "Iteration 590, loss = 1.9432\n",
      "Got 2130 / 10195 correct (20.89%)\n",
      "Iteration 600, loss = 2.0358\n",
      "Got 2126 / 10195 correct (20.85%)\n",
      "Iteration 610, loss = 2.1767\n",
      "Got 2136 / 10195 correct (20.95%)\n",
      "Iteration 620, loss = 1.9709\n",
      "Got 2153 / 10195 correct (21.12%)\n",
      "Iteration 630, loss = 2.0398\n",
      "Got 2148 / 10195 correct (21.07%)\n",
      "Iteration 640, loss = 2.1647\n",
      "Got 2157 / 10195 correct (21.16%)\n",
      "Iteration 650, loss = 2.0652\n",
      "Got 2147 / 10195 correct (21.06%)\n",
      "Iteration 660, loss = 1.9652\n",
      "Got 2150 / 10195 correct (21.09%)\n",
      "Iteration 670, loss = 2.1077\n",
      "Got 2162 / 10195 correct (21.21%)\n",
      "Iteration 680, loss = 2.0762\n",
      "Got 2166 / 10195 correct (21.25%)\n",
      "Iteration 690, loss = 1.9604\n",
      "Got 2169 / 10195 correct (21.28%)\n",
      "Iteration 700, loss = 1.8911\n",
      "Got 2168 / 10195 correct (21.27%)\n",
      "Iteration 710, loss = 1.9756\n",
      "Got 2172 / 10195 correct (21.30%)\n",
      "Iteration 720, loss = 1.9091\n",
      "Got 2181 / 10195 correct (21.39%)\n",
      "Iteration 730, loss = 1.8914\n",
      "Got 2169 / 10195 correct (21.28%)\n",
      "Iteration 740, loss = 2.1251\n",
      "Got 2173 / 10195 correct (21.31%)\n",
      "Iteration 750, loss = 1.8015\n",
      "Got 2170 / 10195 correct (21.28%)\n",
      "Iteration 760, loss = 1.9906\n",
      "Got 2177 / 10195 correct (21.35%)\n",
      "Iteration 770, loss = 1.9653\n",
      "Got 2171 / 10195 correct (21.29%)\n",
      "Iteration 780, loss = 1.9168\n",
      "Got 2173 / 10195 correct (21.31%)\n",
      "Iteration 790, loss = 2.2397\n",
      "Got 2172 / 10195 correct (21.30%)\n",
      "Iteration 800, loss = 1.9590\n",
      "Got 2165 / 10195 correct (21.24%)\n",
      "Iteration 810, loss = 2.0303\n",
      "Got 2155 / 10195 correct (21.14%)\n",
      "Iteration 820, loss = 2.1196\n",
      "Got 2172 / 10195 correct (21.30%)\n",
      "Iteration 830, loss = 1.8726\n",
      "Got 2175 / 10195 correct (21.33%)\n",
      "Iteration 840, loss = 2.1595\n",
      "Got 2173 / 10195 correct (21.31%)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a Three-Layer ConvNet, for use with the\n",
    "    three_layer_convnet function defined above.\n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns a list containing:\n",
    "    - conv_w1: TensorFlow tf.Variable giving weights for the first conv layer\n",
    "    - conv_b1: TensorFlow tf.Variable giving biases for the first conv layer\n",
    "    - conv_w2: TensorFlow tf.Variable giving weights for the second conv layer\n",
    "    - conv_b2: TensorFlow tf.Variable giving biases for the second conv layer\n",
    "    - fc_w: TensorFlow tf.Variable giving weights for the fully-connected layer\n",
    "    - fc_b: TensorFlow tf.Variable giving biases for the fully-connected layer\n",
    "    \"\"\"\n",
    "    params = None\n",
    "    ############################################################################\n",
    "    # TODO: Initialize the parameters of the three-layer network.              #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    channel_1, channel_2, num_classes = 32, 16, 6 #arbitrary choice of channels and num_classes\n",
    "    conv_w1 = tf.Variable(create_matrix_with_kaiming_normal((5, 5, 3, channel_1)), dtype=tf.float32) # (KH1, KW1, 3, channel_1)\n",
    "    conv_b1 = tf.Variable(tf.zeros(channel_1, dtype=tf.float32)) # (channel_1,)\n",
    "    conv_w2 = tf.Variable(create_matrix_with_kaiming_normal((3, 3, channel_1, channel_2)), dtype=tf.float32) # (KH2, KW2, channel_1, channel_2)\n",
    "    conv_b2 = tf.Variable(tf.zeros(channel_2, dtype=tf.float32)) # (channel_2,)\n",
    "    fc_input_dim = 32 * 32 * channel_2 \n",
    "    fc_w = tf.Variable(create_matrix_with_kaiming_normal((fc_input_dim, num_classes)), dtype=tf.float32)\n",
    "    fc_b = tf.Variable(tf.zeros(num_classes, dtype=tf.float32))\n",
    "    params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return params\n",
    "\n",
    "learning_rate = 1e-4\n",
    "train_part2(three_layer_convnet, three_layer_convnet_init, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hXqrHKGUCOg",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Part III: Keras Model Subclassing API\n",
    "\n",
    "Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters. This was fine for a small network, but could quickly become unweildy for a large complex model.\n",
    "\n",
    "Fortunately TensorFlow 2.0 provides higher-level APIs such as `tf.keras` which make it easy to build models out of modular, object-oriented layers. Further, TensorFlow 2.0 uses eager execution that evaluates operations immediately, without explicitly constructing any computational graphs. This makes it easy to write and debug models, and reduces the boilerplate code.\n",
    "\n",
    "In this part of the notebook we will define neural network models using the `tf.keras.Model` API. To implement your own model, you need to do the following:\n",
    "\n",
    "1. Define a new class which subclasses `tf.keras.Model`. Give your class an intuitive name that describes it, like `TwoLayerFC` or `ThreeLayerConvNet`.\n",
    "2. In the initializer `__init__()` for your new class, define all the layers you need as class attributes. The `tf.keras.layers` package provides many common neural-network layers, like `tf.keras.layers.Dense` for fully-connected layers and `tf.keras.layers.Conv2D` for convolutional layers. Under the hood, these layers will construct `Variable` Tensors for any learnable parameters. **Warning**: Don't forget to call `super(YourModelName, self).__init__()` as the first line in your initializer!\n",
    "3. Implement the `call()` method for your class; this implements the forward pass of your model, and defines the *connectivity* of your network. Layers defined in `__init__()` implement `__call__()` so they can be used as function objects that transform input Tensors into output Tensors. Don't define any new layers in `call()`; any layers you want to use in the forward pass should be defined in `__init__()`.\n",
    "\n",
    "After you define your `tf.keras.Model` subclass, you can instantiate it and use it like the model functions from Part II.\n",
    "\n",
    "### Keras Model Subclassing API: Two-Layer Network\n",
    "\n",
    "Here is a concrete example of using the `tf.keras.Model` API to define a two-layer network. There are a few new bits of API to be aware of here:\n",
    "\n",
    "We use an `Initializer` object to set up the initial values of the learnable parameters of the layers; in particular `tf.initializers.VarianceScaling` gives behavior similar to the Kaiming initialization method we used in Part II. You can read more about it here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/initializers/VarianceScaling\n",
    "\n",
    "We construct `tf.keras.layers.Dense` objects to represent the two fully-connected layers of the model. In addition to multiplying their input by a weight matrix and adding a bias vector, these layer can also apply a nonlinearity for you. For the first layer we specify a ReLU activation function by passing `activation='relu'` to the constructor; the second layer uses softmax activation function. Finally, we use `tf.keras.layers.Flatten` to flatten the output from the previous fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "UhKFUoHdUCOh",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 6)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__()        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 6\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 96.62543487548828, Accuracy: 15.625, Val Loss: 102.14210510253906, Val Accuracy: 18.528690338134766\n",
      "Iteration 10, Epoch 1, Loss: 87.73381042480469, Accuracy: 21.875, Val Loss: 85.21002197265625, Val Accuracy: 19.55860710144043\n",
      "Iteration 20, Epoch 1, Loss: 82.72669219970703, Accuracy: 23.214284896850586, Val Loss: 69.49610137939453, Val Accuracy: 24.12947654724121\n",
      "Iteration 30, Epoch 1, Loss: 78.68995666503906, Accuracy: 23.941532135009766, Val Loss: 78.77084350585938, Val Accuracy: 21.196664810180664\n",
      "Iteration 40, Epoch 1, Loss: 76.35467529296875, Accuracy: 24.352134704589844, Val Loss: 76.78191375732422, Val Accuracy: 23.845022201538086\n",
      "Iteration 50, Epoch 1, Loss: 78.9779052734375, Accuracy: 23.71323585510254, Val Loss: 70.71685791015625, Val Accuracy: 23.815595626831055\n",
      "Iteration 60, Epoch 1, Loss: 78.7224349975586, Accuracy: 23.616802215576172, Val Loss: 85.17578887939453, Val Accuracy: 21.53997039794922\n",
      "Iteration 70, Epoch 1, Loss: 78.10658264160156, Accuracy: 23.855634689331055, Val Loss: 67.9995346069336, Val Accuracy: 23.0014705657959\n",
      "Iteration 80, Epoch 1, Loss: 79.11863708496094, Accuracy: 23.842592239379883, Val Loss: 87.72679901123047, Val Accuracy: 27.26827049255371\n",
      "Iteration 90, Epoch 1, Loss: 80.05791473388672, Accuracy: 24.021291732788086, Val Loss: 81.31185150146484, Val Accuracy: 23.44286346435547\n",
      "Iteration 100, Epoch 1, Loss: 79.01303100585938, Accuracy: 24.396657943725586, Val Loss: 74.4392318725586, Val Accuracy: 22.138303756713867\n",
      "Iteration 110, Epoch 1, Loss: 78.916015625, Accuracy: 24.451013565063477, Val Loss: 56.20817947387695, Val Accuracy: 28.239334106445312\n",
      "Iteration 120, Epoch 1, Loss: 77.94419860839844, Accuracy: 24.857954025268555, Val Loss: 64.59294128417969, Val Accuracy: 29.141735076904297\n",
      "Iteration 130, Epoch 1, Loss: 76.56568908691406, Accuracy: 25.190839767456055, Val Loss: 63.72467041015625, Val Accuracy: 26.051986694335938\n",
      "Iteration 140, Epoch 1, Loss: 75.35540771484375, Accuracy: 25.653812408447266, Val Loss: 52.52613067626953, Val Accuracy: 27.503677368164062\n",
      "Iteration 150, Epoch 1, Loss: 75.03380584716797, Accuracy: 25.25869369506836, Val Loss: 77.25409698486328, Val Accuracy: 24.76704216003418\n",
      "Iteration 160, Epoch 1, Loss: 74.06808471679688, Accuracy: 25.494953155517578, Val Loss: 74.1231460571289, Val Accuracy: 30.819028854370117\n",
      "Iteration 170, Epoch 1, Loss: 73.64042663574219, Accuracy: 25.41118621826172, Val Loss: 74.81834411621094, Val Accuracy: 26.08141326904297\n",
      "Iteration 180, Epoch 1, Loss: 73.01282501220703, Accuracy: 25.630178451538086, Val Loss: 59.81000518798828, Val Accuracy: 25.443845748901367\n",
      "Iteration 190, Epoch 1, Loss: 72.22474670410156, Accuracy: 25.826242446899414, Val Loss: 56.227622985839844, Val Accuracy: 27.278078079223633\n",
      "Iteration 200, Epoch 1, Loss: 72.1222152709961, Accuracy: 25.792911529541016, Val Loss: 86.53939819335938, Val Accuracy: 24.659147262573242\n",
      "Iteration 210, Epoch 1, Loss: 71.83251953125, Accuracy: 25.910842895507812, Val Loss: 60.115943908691406, Val Accuracy: 27.003435134887695\n",
      "Iteration 220, Epoch 1, Loss: 71.90282440185547, Accuracy: 25.869625091552734, Val Loss: 70.72799682617188, Val Accuracy: 28.81804656982422\n",
      "Iteration 230, Epoch 1, Loss: 71.80000305175781, Accuracy: 25.89285659790039, Val Loss: 83.85107421875, Val Accuracy: 27.87640953063965\n",
      "Iteration 240, Epoch 1, Loss: 71.75997161865234, Accuracy: 25.940093994140625, Val Loss: 74.85139465332031, Val Accuracy: 22.20696449279785\n",
      "Iteration 250, Epoch 1, Loss: 71.48816680908203, Accuracy: 26.04581642150879, Val Loss: 72.42378997802734, Val Accuracy: 26.17949867248535\n",
      "Iteration 260, Epoch 1, Loss: 71.40801239013672, Accuracy: 25.96384048461914, Val Loss: 57.17415237426758, Val Accuracy: 28.160863876342773\n",
      "Iteration 270, Epoch 1, Loss: 71.09744262695312, Accuracy: 25.96863555908203, Val Loss: 63.5256462097168, Val Accuracy: 31.73124122619629\n",
      "Iteration 280, Epoch 1, Loss: 70.74176025390625, Accuracy: 25.984207153320312, Val Loss: 59.336669921875, Val Accuracy: 29.9166259765625\n",
      "Iteration 290, Epoch 1, Loss: 70.15225982666016, Accuracy: 26.19738006591797, Val Loss: 56.0751953125, Val Accuracy: 28.01373291015625\n",
      "Iteration 300, Epoch 1, Loss: 69.61022186279297, Accuracy: 26.292566299438477, Val Loss: 53.7025146484375, Val Accuracy: 28.082393646240234\n",
      "Iteration 310, Epoch 1, Loss: 69.7891845703125, Accuracy: 26.205787658691406, Val Loss: 79.50062561035156, Val Accuracy: 25.718488693237305\n",
      "Iteration 320, Epoch 1, Loss: 69.60127258300781, Accuracy: 26.231502532958984, Val Loss: 56.102272033691406, Val Accuracy: 24.659147262573242\n",
      "Iteration 330, Epoch 1, Loss: 69.17205810546875, Accuracy: 26.222620010375977, Val Loss: 66.44009399414062, Val Accuracy: 27.091711044311523\n",
      "Iteration 340, Epoch 1, Loss: 68.92745971679688, Accuracy: 26.23716926574707, Val Loss: 68.63165283203125, Val Accuracy: 24.865129470825195\n",
      "Iteration 350, Epoch 1, Loss: 68.44719696044922, Accuracy: 26.371082305908203, Val Loss: 50.131492614746094, Val Accuracy: 29.092693328857422\n",
      "Iteration 360, Epoch 1, Loss: 68.38276672363281, Accuracy: 26.307132720947266, Val Loss: 71.43571472167969, Val Accuracy: 30.613046646118164\n",
      "Iteration 370, Epoch 1, Loss: 68.03182220458984, Accuracy: 26.38140106201172, Val Loss: 46.69961166381836, Val Accuracy: 28.631681442260742\n",
      "Iteration 380, Epoch 1, Loss: 67.43433380126953, Accuracy: 26.554298400878906, Val Loss: 52.5501823425293, Val Accuracy: 25.277095794677734\n",
      "Iteration 390, Epoch 1, Loss: 67.05927276611328, Accuracy: 26.65041732788086, Val Loss: 54.24980545043945, Val Accuracy: 31.701814651489258\n",
      "Iteration 400, Epoch 1, Loss: 67.06245422363281, Accuracy: 26.60146713256836, Val Loss: 50.18866729736328, Val Accuracy: 28.219715118408203\n",
      "Iteration 410, Epoch 1, Loss: 66.9547119140625, Accuracy: 26.566303253173828, Val Loss: 71.40579223632812, Val Accuracy: 22.86414909362793\n",
      "Iteration 420, Epoch 1, Loss: 66.74723815917969, Accuracy: 26.647863388061523, Val Loss: 44.159767150878906, Val Accuracy: 29.965669631958008\n",
      "Iteration 430, Epoch 1, Loss: 66.42627716064453, Accuracy: 26.703886032104492, Val Loss: 42.62677001953125, Val Accuracy: 29.97547721862793\n",
      "Iteration 440, Epoch 1, Loss: 66.01223754882812, Accuracy: 26.842403411865234, Val Loss: 72.35578155517578, Val Accuracy: 24.992643356323242\n",
      "Iteration 450, Epoch 1, Loss: 65.8642349243164, Accuracy: 26.825803756713867, Val Loss: 70.2861557006836, Val Accuracy: 25.865619659423828\n",
      "Iteration 460, Epoch 1, Loss: 65.60713195800781, Accuracy: 26.914995193481445, Val Loss: 68.38319396972656, Val Accuracy: 25.95389747619629\n",
      "Iteration 470, Epoch 1, Loss: 65.48965454101562, Accuracy: 26.9572696685791, Val Loss: 52.92429733276367, Val Accuracy: 29.769495010375977\n",
      "Iteration 480, Epoch 1, Loss: 65.2209701538086, Accuracy: 27.007535934448242, Val Loss: 38.72472381591797, Val Accuracy: 31.064247131347656\n",
      "Iteration 490, Epoch 1, Loss: 64.95008087158203, Accuracy: 27.087575912475586, Val Loss: 67.03455352783203, Val Accuracy: 30.00490379333496\n",
      "Iteration 500, Epoch 1, Loss: 64.82655334472656, Accuracy: 27.092689514160156, Val Loss: 54.12282180786133, Val Accuracy: 27.385974884033203\n",
      "Iteration 510, Epoch 1, Loss: 64.61974334716797, Accuracy: 27.13123893737793, Val Loss: 56.23417282104492, Val Accuracy: 27.35654640197754\n",
      "Iteration 520, Epoch 1, Loss: 64.41231536865234, Accuracy: 27.153310775756836, Val Loss: 58.55449295043945, Val Accuracy: 28.523784637451172\n",
      "Iteration 530, Epoch 1, Loss: 64.11231994628906, Accuracy: 27.206920623779297, Val Loss: 43.17583084106445, Val Accuracy: 32.92790603637695\n",
      "Iteration 540, Epoch 1, Loss: 63.94884490966797, Accuracy: 27.235443115234375, Val Loss: 57.30881881713867, Val Accuracy: 31.564491271972656\n",
      "Iteration 550, Epoch 1, Loss: 63.85236358642578, Accuracy: 27.200544357299805, Val Loss: 50.840702056884766, Val Accuracy: 29.357528686523438\n",
      "Iteration 560, Epoch 1, Loss: 63.58705139160156, Accuracy: 27.230947494506836, Val Loss: 51.11304473876953, Val Accuracy: 30.43648910522461\n",
      "Iteration 570, Epoch 1, Loss: 63.531463623046875, Accuracy: 27.224716186523438, Val Loss: 70.98484802246094, Val Accuracy: 23.73712730407715\n",
      "Iteration 580, Epoch 1, Loss: 63.42889404296875, Accuracy: 27.25096893310547, Val Loss: 49.16344451904297, Val Accuracy: 32.65325927734375\n",
      "Iteration 590, Epoch 1, Loss: 63.19792938232422, Accuracy: 27.318632125854492, Val Loss: 62.141807556152344, Val Accuracy: 26.826875686645508\n",
      "Iteration 600, Epoch 1, Loss: 63.01319885253906, Accuracy: 27.31125259399414, Val Loss: 50.48257827758789, Val Accuracy: 30.54438591003418\n",
      "Iteration 610, Epoch 1, Loss: 62.78078842163086, Accuracy: 27.324569702148438, Val Loss: 67.78974151611328, Val Accuracy: 26.218732833862305\n",
      "Iteration 620, Epoch 1, Loss: 62.645721435546875, Accuracy: 27.362621307373047, Val Loss: 59.294898986816406, Val Accuracy: 30.044137954711914\n",
      "Iteration 630, Epoch 1, Loss: 62.46460723876953, Accuracy: 27.40689468383789, Val Loss: 49.14375686645508, Val Accuracy: 33.035804748535156\n",
      "Iteration 640, Epoch 1, Loss: 62.24323654174805, Accuracy: 27.452224731445312, Val Loss: 49.323387145996094, Val Accuracy: 29.25943946838379\n",
      "Iteration 650, Epoch 1, Loss: 62.114036560058594, Accuracy: 27.467357635498047, Val Loss: 45.958370208740234, Val Accuracy: 30.72093963623047\n",
      "Iteration 660, Epoch 1, Loss: 62.03730010986328, Accuracy: 27.48676300048828, Val Loss: 69.674072265625, Val Accuracy: 29.180971145629883\n",
      "Iteration 670, Epoch 1, Loss: 61.778690338134766, Accuracy: 27.549833297729492, Val Loss: 37.28921890258789, Val Accuracy: 30.740558624267578\n",
      "Iteration 680, Epoch 1, Loss: 61.41819381713867, Accuracy: 27.670705795288086, Val Loss: 44.805538177490234, Val Accuracy: 31.809711456298828\n",
      "Iteration 690, Epoch 1, Loss: 61.20714569091797, Accuracy: 27.70441436767578, Val Loss: 60.62151336669922, Val Accuracy: 30.269739151000977\n",
      "Iteration 700, Epoch 1, Loss: 61.08985900878906, Accuracy: 27.739391326904297, Val Loss: 48.748050689697266, Val Accuracy: 31.95684242248535\n",
      "Iteration 710, Epoch 1, Loss: 60.86009979248047, Accuracy: 27.801952362060547, Val Loss: 45.60396194458008, Val Accuracy: 29.47523307800293\n",
      "Iteration 720, Epoch 1, Loss: 60.635169982910156, Accuracy: 27.903953552246094, Val Loss: 50.42223358154297, Val Accuracy: 28.494361877441406\n",
      "Iteration 730, Epoch 1, Loss: 60.541114807128906, Accuracy: 27.94117546081543, Val Loss: 48.485137939453125, Val Accuracy: 31.172143936157227\n",
      "Iteration 740, Epoch 1, Loss: 60.42547607421875, Accuracy: 27.96685028076172, Val Loss: 43.84125518798828, Val Accuracy: 31.68219566345215\n",
      "Iteration 750, Epoch 1, Loss: 60.22050857543945, Accuracy: 28.018892288208008, Val Loss: 46.860172271728516, Val Accuracy: 30.230504989624023\n",
      "Iteration 760, Epoch 1, Loss: 60.05393981933594, Accuracy: 28.032604217529297, Val Loss: 53.668601989746094, Val Accuracy: 30.740558624267578\n",
      "Iteration 770, Epoch 1, Loss: 59.90531921386719, Accuracy: 28.07636260986328, Val Loss: 39.69017791748047, Val Accuracy: 30.564001083374023\n",
      "Iteration 780, Epoch 1, Loss: 59.74908447265625, Accuracy: 28.0889892578125, Val Loss: 53.66022491455078, Val Accuracy: 30.201080322265625\n",
      "Iteration 790, Epoch 1, Loss: 59.5767822265625, Accuracy: 28.142776489257812, Val Loss: 66.31661224365234, Val Accuracy: 30.917116165161133\n",
      "Iteration 800, Epoch 1, Loss: 59.38138961791992, Accuracy: 28.197175979614258, Val Loss: 42.425010681152344, Val Accuracy: 28.729768753051758\n",
      "Iteration 810, Epoch 1, Loss: 59.19917678833008, Accuracy: 28.25215721130371, Val Loss: 52.507896423339844, Val Accuracy: 30.622854232788086\n",
      "Iteration 820, Epoch 1, Loss: 59.05938720703125, Accuracy: 28.284866333007812, Val Loss: 43.32559585571289, Val Accuracy: 31.584110260009766\n",
      "Iteration 830, Epoch 1, Loss: 58.84758758544922, Accuracy: 28.30550765991211, Val Loss: 60.349586486816406, Val Accuracy: 28.01373291015625\n",
      "Iteration 840, Epoch 1, Loss: 58.71771240234375, Accuracy: 28.310791015625, Val Loss: 39.40160369873047, Val Accuracy: 30.57381248474121\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 6\n",
    "learning_rate = 1e-4\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJbRTVz_UCOi"
   },
   "source": [
    "### Keras Model Subclassing API: Three-Layer ConvNet\n",
    "Now it's your turn to implement a three-layer ConvNet using the `tf.keras.Model` API. Your model should have the same architecture used in Part II:\n",
    "\n",
    "1. Convolutional layer with 5 x 5 kernels, with zero-padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 3 x 3 kernels, with zero-padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer to give class scores\n",
    "6. Softmax nonlinearity\n",
    "\n",
    "You should initialize the weights of your network using the same initialization method as was used in the two-layer network above.\n",
    "\n",
    "**Hint**: Refer to the documentation for `tf.keras.layers.Conv2D` and `tf.keras.layers.Dense`:\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "Iv9VpLmLUCOi"
   },
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=channel_1,\n",
    "            kernel_size=(5, 5),\n",
    "            padding='same',\n",
    "            activation=None,\n",
    "            kernel_initializer=initializer,\n",
    "            bias_initializer='zeros')\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=channel_2,\n",
    "            kernel_size=(3, 3),\n",
    "            padding='same',\n",
    "            activation=None,\n",
    "            kernel_initializer=initializer,\n",
    "            bias_initializer='zeros')\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(\n",
    "            units=num_classes,\n",
    "            activation='softmax',\n",
    "            kernel_initializer=initializer,\n",
    "            bias_initializer='zeros')\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        x = tf.transpose(x, perm=[0, 2, 3, 1])  # NCHW -> NHWC for TF conv layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.flatten(x)\n",
    "        scores = self.fc(x)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7c_ph2AUCOk"
   },
   "source": [
    "Once you complete the implementation of the `ThreeLayerConvNet` above you can run the following to ensure that your implementation does not crash and produces outputs of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "keras_model_output_shape"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 6)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 6\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZPzUm41UCOm"
   },
   "source": [
    "### Keras Model Subclassing API: Eager Training\n",
    "\n",
    "While keras models have a builtin training loop (using the `model.fit`), sometimes you need more customization. Here's an example, of a training loop implemented with eager execution.\n",
    "\n",
    "In particular, notice `tf.GradientTape`. Automatic differentiation is used in the backend for implementing backpropagation in frameworks like TensorFlow. During eager execution, `tf.GradientTape` is used to trace operations for computing gradients later. A particular `tf.GradientTape` can only compute one gradient; subsequent calls to tape will throw a runtime error. \n",
    "\n",
    "TensorFlow 2.0 ships with easy-to-use built-in metrics under `tf.keras.metrics` module. Each metric is an object, and we can use `update_state()` to add observations and `reset_state()` to clear all observations. We can get the current result of a metric by calling `result()` on the metric object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "sEdNeHF7UCOn",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CINIC-10 training set and periodically checks\n",
    "    accuracy on the CINIC-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "\n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                # Update the metrics\n",
    "                train_loss.update_state(loss)\n",
    "                train_accuracy.update_state(y_np, scores)\n",
    "\n",
    "                if t % print_every == 0:\n",
    "                    val_loss.reset_states()\n",
    "                    val_accuracy.reset_states()\n",
    "                    for test_x, test_y in val_dset:\n",
    "                        # During validation at end of epoch, training set to False\n",
    "                        prediction = model(test_x, training=False)\n",
    "                        t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                        val_loss.update_state(t_loss)\n",
    "                        val_accuracy.update_state(test_y, prediction)\n",
    "\n",
    "                    template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                    print(template.format(\n",
    "                        t, epoch + 1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result() * 100,\n",
    "                        val_loss.result(),\n",
    "                        val_accuracy.result() * 100\n",
    "                    ))\n",
    "                t += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4jsppftUCOo"
   },
   "source": [
    "### Keras Model Subclassing API: Train a Two-Layer Network\n",
    "We can now use the tools defined above to train a two-layer network on CINIC-10. We define the `model_init_fn` and `optimizer_init_fn` that construct the model and optimizer respectively when called. Here we want to train the model using stochastic gradient descent with no momentum, so we construct a `tf.keras.optimizers.SGD` function; you can [read about it here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD).\n",
    "\n",
    "You don't need to tune any hyperparameters here, but you should achieve validation accuracies above 30% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "GFx-YT8pUCOp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 91.34944915771484, Accuracy: 10.9375, Val Loss: 56.33817672729492, Val Accuracy: 15.743011474609375\n",
      "Iteration 10, Epoch 1, Loss: 46.9233283996582, Accuracy: 14.772727012634277, Val Loss: 36.8912239074707, Val Accuracy: 18.73467445373535\n",
      "Iteration 20, Epoch 1, Loss: 41.011634826660156, Accuracy: 17.931547164916992, Val Loss: 31.84600830078125, Val Accuracy: 19.794017791748047\n",
      "Iteration 30, Epoch 1, Loss: 37.39537811279297, Accuracy: 18.598791122436523, Val Loss: 29.59173583984375, Val Accuracy: 20.62775993347168\n",
      "Iteration 40, Epoch 1, Loss: 35.06196212768555, Accuracy: 19.474084854125977, Val Loss: 26.47980308532715, Val Accuracy: 20.892595291137695\n",
      "Iteration 50, Epoch 1, Loss: 33.46692657470703, Accuracy: 19.577205657958984, Val Loss: 24.708444595336914, Val Accuracy: 20.60814094543457\n",
      "Iteration 60, Epoch 1, Loss: 31.862350463867188, Accuracy: 19.87704849243164, Val Loss: 23.219335556030273, Val Accuracy: 21.245708465576172\n",
      "Iteration 70, Epoch 1, Loss: 30.731782913208008, Accuracy: 20.048416137695312, Val Loss: 22.5046329498291, Val Accuracy: 20.9710636138916\n",
      "Iteration 80, Epoch 1, Loss: 29.45172691345215, Accuracy: 20.254629135131836, Val Loss: 21.12991714477539, Val Accuracy: 20.588523864746094\n",
      "Iteration 90, Epoch 1, Loss: 28.42831039428711, Accuracy: 20.243818283081055, Val Loss: 20.445547103881836, Val Accuracy: 21.87346649169922\n",
      "Iteration 100, Epoch 1, Loss: 27.505943298339844, Accuracy: 20.56002426147461, Val Loss: 19.10952377319336, Val Accuracy: 21.775379180908203\n",
      "Iteration 110, Epoch 1, Loss: 26.813430786132812, Accuracy: 20.52364730834961, Val Loss: 18.19035530090332, Val Accuracy: 22.167728424072266\n",
      "Iteration 120, Epoch 1, Loss: 26.03252601623535, Accuracy: 20.738636016845703, Val Loss: 17.223453521728516, Val Accuracy: 22.481609344482422\n",
      "Iteration 130, Epoch 1, Loss: 25.383501052856445, Accuracy: 20.81345558166504, Val Loss: 16.435441970825195, Val Accuracy: 22.609121322631836\n",
      "Iteration 140, Epoch 1, Loss: 24.83740997314453, Accuracy: 20.58953857421875, Val Loss: 15.605073928833008, Val Accuracy: 23.452672958374023\n",
      "Iteration 150, Epoch 1, Loss: 24.208993911743164, Accuracy: 20.757450103759766, Val Loss: 14.747474670410156, Val Accuracy: 22.481609344482422\n",
      "Iteration 160, Epoch 1, Loss: 23.63694953918457, Accuracy: 20.797748565673828, Val Loss: 13.98973560333252, Val Accuracy: 22.471799850463867\n",
      "Iteration 170, Epoch 1, Loss: 23.0079402923584, Accuracy: 21.052631378173828, Val Loss: 13.369766235351562, Val Accuracy: 22.432565689086914\n",
      "Iteration 180, Epoch 1, Loss: 22.45140838623047, Accuracy: 21.14986228942871, Val Loss: 12.779997825622559, Val Accuracy: 21.804805755615234\n",
      "Iteration 190, Epoch 1, Loss: 21.938926696777344, Accuracy: 21.106021881103516, Val Loss: 12.217206001281738, Val Accuracy: 21.942129135131836\n",
      "Iteration 200, Epoch 1, Loss: 21.462724685668945, Accuracy: 21.050994873046875, Val Loss: 11.575685501098633, Val Accuracy: 22.442373275756836\n",
      "Iteration 210, Epoch 1, Loss: 21.007648468017578, Accuracy: 21.104856491088867, Val Loss: 10.952385902404785, Val Accuracy: 22.099069595336914\n",
      "Iteration 220, Epoch 1, Loss: 20.550569534301758, Accuracy: 21.125564575195312, Val Loss: 10.44049072265625, Val Accuracy: 22.324668884277344\n",
      "Iteration 230, Epoch 1, Loss: 20.106660842895508, Accuracy: 21.063310623168945, Val Loss: 9.951518058776855, Val Accuracy: 22.167728424072266\n",
      "Iteration 240, Epoch 1, Loss: 19.65497398376465, Accuracy: 21.103473663330078, Val Loss: 9.529081344604492, Val Accuracy: 22.11868667602539\n",
      "Iteration 250, Epoch 1, Loss: 19.224166870117188, Accuracy: 21.208913803100586, Val Loss: 9.095282554626465, Val Accuracy: 22.295242309570312\n",
      "Iteration 260, Epoch 1, Loss: 18.827232360839844, Accuracy: 21.240421295166016, Val Loss: 8.668712615966797, Val Accuracy: 21.618440628051758\n",
      "Iteration 270, Epoch 1, Loss: 18.452117919921875, Accuracy: 21.235008239746094, Val Loss: 8.296010971069336, Val Accuracy: 21.43207550048828\n",
      "Iteration 280, Epoch 1, Loss: 18.07241439819336, Accuracy: 21.291147232055664, Val Loss: 7.936227321624756, Val Accuracy: 21.412458419799805\n",
      "Iteration 290, Epoch 1, Loss: 17.721036911010742, Accuracy: 21.33268928527832, Val Loss: 7.550807952880859, Val Accuracy: 21.353605270385742\n",
      "Iteration 300, Epoch 1, Loss: 17.37577247619629, Accuracy: 21.257266998291016, Val Loss: 7.216261863708496, Val Accuracy: 20.980873107910156\n",
      "Iteration 310, Epoch 1, Loss: 17.042354583740234, Accuracy: 21.22688865661621, Val Loss: 6.915532112121582, Val Accuracy: 20.784698486328125\n",
      "Iteration 320, Epoch 1, Loss: 16.717010498046875, Accuracy: 21.188669204711914, Val Loss: 6.624423980712891, Val Accuracy: 20.588523864746094\n",
      "Iteration 330, Epoch 1, Loss: 16.41560935974121, Accuracy: 21.15275764465332, Val Loss: 6.345009803771973, Val Accuracy: 20.461009979248047\n",
      "Iteration 340, Epoch 1, Loss: 16.114593505859375, Accuracy: 21.114370346069336, Val Loss: 6.0878801345825195, Val Accuracy: 20.30406951904297\n",
      "Iteration 350, Epoch 1, Loss: 15.82320785522461, Accuracy: 21.17165184020996, Val Loss: 5.839507102966309, Val Accuracy: 19.852869033813477\n",
      "Iteration 360, Epoch 1, Loss: 15.545448303222656, Accuracy: 21.139196395874023, Val Loss: 5.602234840393066, Val Accuracy: 19.80382537841797\n",
      "Iteration 370, Epoch 1, Loss: 15.274773597717285, Accuracy: 21.091644287109375, Val Loss: 5.383060455322266, Val Accuracy: 19.431093215942383\n",
      "Iteration 380, Epoch 1, Loss: 15.010549545288086, Accuracy: 21.062992095947266, Val Loss: 5.175994873046875, Val Accuracy: 19.411476135253906\n",
      "Iteration 390, Epoch 1, Loss: 14.767783164978027, Accuracy: 21.075767517089844, Val Loss: 4.975563049316406, Val Accuracy: 19.489946365356445\n",
      "Iteration 400, Epoch 1, Loss: 14.527740478515625, Accuracy: 21.006078720092773, Val Loss: 4.796462535858154, Val Accuracy: 19.127023696899414\n",
      "Iteration 410, Epoch 1, Loss: 14.290885925292969, Accuracy: 21.012012481689453, Val Loss: 4.624741077423096, Val Accuracy: 19.136831283569336\n",
      "Iteration 420, Epoch 1, Loss: 14.060059547424316, Accuracy: 20.895191192626953, Val Loss: 4.486330986022949, Val Accuracy: 18.90142250061035\n",
      "Iteration 430, Epoch 1, Loss: 13.83912467956543, Accuracy: 20.874420166015625, Val Loss: 4.337535858154297, Val Accuracy: 18.852378845214844\n",
      "Iteration 440, Epoch 1, Loss: 13.62027359008789, Accuracy: 20.868764877319336, Val Loss: 4.209408760070801, Val Accuracy: 18.960275650024414\n",
      "Iteration 450, Epoch 1, Loss: 13.411543846130371, Accuracy: 20.79753303527832, Val Loss: 4.0845513343811035, Val Accuracy: 18.76409912109375\n",
      "Iteration 460, Epoch 1, Loss: 13.207134246826172, Accuracy: 20.790401458740234, Val Loss: 3.98876690864563, Val Accuracy: 19.0289363861084\n",
      "Iteration 470, Epoch 1, Loss: 13.00942325592041, Accuracy: 20.750396728515625, Val Loss: 3.8687186241149902, Val Accuracy: 18.960275650024414\n",
      "Iteration 480, Epoch 1, Loss: 12.816667556762695, Accuracy: 20.783523559570312, Val Loss: 3.7723987102508545, Val Accuracy: 19.058361053466797\n",
      "Iteration 490, Epoch 1, Loss: 12.629714012145996, Accuracy: 20.796207427978516, Val Loss: 3.679208755493164, Val Accuracy: 18.803333282470703\n",
      "Iteration 500, Epoch 1, Loss: 12.452767372131348, Accuracy: 20.77719497680664, Val Loss: 3.587482452392578, Val Accuracy: 18.73467445373535\n",
      "Iteration 510, Epoch 1, Loss: 12.280680656433105, Accuracy: 20.72529411315918, Val Loss: 3.4984161853790283, Val Accuracy: 18.842571258544922\n",
      "Iteration 520, Epoch 1, Loss: 12.111000061035156, Accuracy: 20.678382873535156, Val Loss: 3.4255967140197754, Val Accuracy: 18.636587142944336\n",
      "Iteration 530, Epoch 1, Loss: 11.948355674743652, Accuracy: 20.647951126098633, Val Loss: 3.352156162261963, Val Accuracy: 18.656204223632812\n",
      "Iteration 540, Epoch 1, Loss: 11.787168502807617, Accuracy: 20.64463996887207, Val Loss: 3.2928478717803955, Val Accuracy: 18.715057373046875\n",
      "Iteration 550, Epoch 1, Loss: 11.6312837600708, Accuracy: 20.630104064941406, Val Loss: 3.232515811920166, Val Accuracy: 18.744482040405273\n",
      "Iteration 560, Epoch 1, Loss: 11.480647087097168, Accuracy: 20.554811477661133, Val Loss: 3.176191806793213, Val Accuracy: 18.93084716796875\n",
      "Iteration 570, Epoch 1, Loss: 11.334102630615234, Accuracy: 20.514995574951172, Val Loss: 3.116865634918213, Val Accuracy: 18.852378845214844\n",
      "Iteration 580, Epoch 1, Loss: 11.192550659179688, Accuracy: 20.455034255981445, Val Loss: 3.0614895820617676, Val Accuracy: 18.940656661987305\n",
      "Iteration 590, Epoch 1, Loss: 11.054920196533203, Accuracy: 20.391815185546875, Val Loss: 3.0102972984313965, Val Accuracy: 18.95046615600586\n",
      "Iteration 600, Epoch 1, Loss: 10.920149803161621, Accuracy: 20.374895095825195, Val Loss: 2.9689178466796875, Val Accuracy: 19.019126892089844\n",
      "Iteration 610, Epoch 1, Loss: 10.791154861450195, Accuracy: 20.361087799072266, Val Loss: 2.923421859741211, Val Accuracy: 19.14664077758789\n",
      "Iteration 620, Epoch 1, Loss: 10.66320514678955, Accuracy: 20.342693328857422, Val Loss: 2.8861427307128906, Val Accuracy: 19.14664077758789\n",
      "Iteration 630, Epoch 1, Loss: 10.539687156677246, Accuracy: 20.317453384399414, Val Loss: 2.848417043685913, Val Accuracy: 18.911230087280273\n",
      "Iteration 640, Epoch 1, Loss: 10.418622016906738, Accuracy: 20.324687957763672, Val Loss: 2.8122448921203613, Val Accuracy: 18.93084716796875\n",
      "Iteration 650, Epoch 1, Loss: 10.30198860168457, Accuracy: 20.300498962402344, Val Loss: 2.7780709266662598, Val Accuracy: 18.832761764526367\n",
      "Iteration 660, Epoch 1, Loss: 10.18787670135498, Accuracy: 20.232128143310547, Val Loss: 2.744615316390991, Val Accuracy: 18.8621883392334\n",
      "Iteration 670, Epoch 1, Loss: 10.076887130737305, Accuracy: 20.221683502197266, Val Loss: 2.70780611038208, Val Accuracy: 18.783716201782227\n",
      "Iteration 680, Epoch 1, Loss: 9.966922760009766, Accuracy: 20.204662322998047, Val Loss: 2.6818125247955322, Val Accuracy: 18.822952270507812\n",
      "Iteration 690, Epoch 1, Loss: 9.86168098449707, Accuracy: 20.18134880065918, Val Loss: 2.6541903018951416, Val Accuracy: 18.76409912109375\n",
      "Iteration 700, Epoch 1, Loss: 9.757857322692871, Accuracy: 20.172075271606445, Val Loss: 2.631927728652954, Val Accuracy: 18.489456176757812\n",
      "Iteration 710, Epoch 1, Loss: 9.656867980957031, Accuracy: 20.13669204711914, Val Loss: 2.6047940254211426, Val Accuracy: 18.62677764892578\n",
      "Iteration 720, Epoch 1, Loss: 9.558833122253418, Accuracy: 20.136962890625, Val Loss: 2.577859401702881, Val Accuracy: 18.51888084411621\n",
      "Iteration 730, Epoch 1, Loss: 9.462660789489746, Accuracy: 20.11798858642578, Val Loss: 2.553948163986206, Val Accuracy: 18.499263763427734\n",
      "Iteration 740, Epoch 1, Loss: 9.36860179901123, Accuracy: 20.07211685180664, Val Loss: 2.5319066047668457, Val Accuracy: 18.50907325744629\n",
      "Iteration 750, Epoch 1, Loss: 9.276407241821289, Accuracy: 20.0753173828125, Val Loss: 2.512291431427002, Val Accuracy: 18.6954402923584\n",
      "Iteration 760, Epoch 1, Loss: 9.188009262084961, Accuracy: 20.0496883392334, Val Loss: 2.4896488189697266, Val Accuracy: 18.656204223632812\n",
      "Iteration 770, Epoch 1, Loss: 9.098875045776367, Accuracy: 20.05714988708496, Val Loss: 2.471911907196045, Val Accuracy: 18.803333282470703\n",
      "Iteration 780, Epoch 1, Loss: 9.014354705810547, Accuracy: 20.04041290283203, Val Loss: 2.4522387981414795, Val Accuracy: 18.852378845214844\n",
      "Iteration 790, Epoch 1, Loss: 8.931703567504883, Accuracy: 20.000394821166992, Val Loss: 2.4359130859375, Val Accuracy: 18.773908615112305\n",
      "Iteration 800, Epoch 1, Loss: 8.850741386413574, Accuracy: 19.975032806396484, Val Loss: 2.4165613651275635, Val Accuracy: 18.822952270507812\n",
      "Iteration 810, Epoch 1, Loss: 8.771377563476562, Accuracy: 19.954147338867188, Val Loss: 2.3985097408294678, Val Accuracy: 18.6954402923584\n",
      "Iteration 820, Epoch 1, Loss: 8.693414688110352, Accuracy: 19.950897216796875, Val Loss: 2.384312629699707, Val Accuracy: 18.666011810302734\n",
      "Iteration 830, Epoch 1, Loss: 8.617135047912598, Accuracy: 19.942089080810547, Val Loss: 2.370709180831909, Val Accuracy: 18.577733993530273\n",
      "Iteration 840, Epoch 1, Loss: 8.54317855834961, Accuracy: 19.918622970581055, Val Loss: 2.3551740646362305, Val Accuracy: 18.46002960205078\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 15\n",
    "channel_1, channel_2, channel_3 = 64, 128, 256\n",
    "num_classes = 6\n",
    "\n",
    "def model_init_fn():\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(3, 32, 32)),\n",
    "        tf.keras.layers.Permute((2, 3, 1)),  # NCHW -> NHWC\n",
    "        tf.keras.layers.RandomFlip('horizontal'),\n",
    "        tf.keras.layers.RandomRotation(0.05),\n",
    "        tf.keras.layers.Conv2D(channel_1, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(channel_1, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(2),\n",
    "        tf.keras.layers.Conv2D(channel_2, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(channel_2, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(2),\n",
    "        tf.keras.layers.Conv2D(channel_3, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(256, activation='relu', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "    ])\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=0.95, momentum=0.0, epsilon=1e-7)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztd_ntb0UCOq"
   },
   "source": [
    "### Keras Model Subclassing  API: Train a Three-Layer ConvNet\n",
    "Here you should use the tools we've defined above to train a three-layer ConvNet on CINIC-10. Your ConvNet should use 32 filters in the first convolutional layer and 16 filters in the second layer.\n",
    "\n",
    "To train the model you should use gradient descent with Nesterov momentum 0.9.  \n",
    "\n",
    "**HINT**: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "keras_model_accuracy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 55.43280029296875, Accuracy: 17.1875, Val Loss: 218.27096557617188, Val Accuracy: 16.929868698120117\n",
      "Iteration 10, Epoch 1, Loss: 32.809722900390625, Accuracy: 17.471590042114258, Val Loss: 1.792354941368103, Val Accuracy: 16.58656120300293\n",
      "Iteration 20, Epoch 1, Loss: 18.04017448425293, Accuracy: 16.220239639282227, Val Loss: 1.7917680740356445, Val Accuracy: 16.6650333404541\n",
      "Iteration 30, Epoch 1, Loss: 12.798770904541016, Accuracy: 15.625, Val Loss: 1.7917680740356445, Val Accuracy: 16.6650333404541\n",
      "Iteration 40, Epoch 1, Loss: 10.11412525177002, Accuracy: 15.891768455505371, Val Loss: 1.7917684316635132, Val Accuracy: 16.6650333404541\n",
      "Iteration 50, Epoch 1, Loss: 8.482284545898438, Accuracy: 15.870099067687988, Val Loss: 1.7917699813842773, Val Accuracy: 16.674840927124023\n",
      "Iteration 60, Epoch 1, Loss: 7.385487079620361, Accuracy: 15.625, Val Loss: 1.7917696237564087, Val Accuracy: 16.645414352416992\n",
      "Iteration 70, Epoch 1, Loss: 6.597649097442627, Accuracy: 15.757041931152344, Val Loss: 1.7917680740356445, Val Accuracy: 16.674840927124023\n",
      "Iteration 80, Epoch 1, Loss: 6.0043134689331055, Accuracy: 15.895061492919922, Val Loss: 1.7917693853378296, Val Accuracy: 16.674840927124023\n",
      "Iteration 90, Epoch 1, Loss: 5.541402816772461, Accuracy: 15.745192527770996, Val Loss: 1.7917712926864624, Val Accuracy: 16.674840927124023\n",
      "Iteration 100, Epoch 1, Loss: 5.170124530792236, Accuracy: 15.887994766235352, Val Loss: 1.7917741537094116, Val Accuracy: 16.645414352416992\n",
      "Iteration 110, Epoch 1, Loss: 4.865778923034668, Accuracy: 16.131757736206055, Val Loss: 1.791774034500122, Val Accuracy: 16.645414352416992\n",
      "Iteration 120, Epoch 1, Loss: 4.6117377281188965, Accuracy: 16.231922149658203, Val Loss: 1.791775107383728, Val Accuracy: 16.645414352416992\n",
      "Iteration 130, Epoch 1, Loss: 4.396485328674316, Accuracy: 16.304866790771484, Val Loss: 1.791772484779358, Val Accuracy: 16.645414352416992\n",
      "Iteration 140, Epoch 1, Loss: 4.211765766143799, Accuracy: 16.312057495117188, Val Loss: 1.7917711734771729, Val Accuracy: 16.645414352416992\n",
      "Iteration 150, Epoch 1, Loss: 4.0514936447143555, Accuracy: 16.338991165161133, Val Loss: 1.7917693853378296, Val Accuracy: 16.645414352416992\n",
      "Iteration 160, Epoch 1, Loss: 3.9111218452453613, Accuracy: 16.430511474609375, Val Loss: 1.7917699813842773, Val Accuracy: 16.645414352416992\n",
      "Iteration 170, Epoch 1, Loss: 3.7871925830841064, Accuracy: 16.474781036376953, Val Loss: 1.7917735576629639, Val Accuracy: 16.645414352416992\n",
      "Iteration 180, Epoch 1, Loss: 3.6769533157348633, Accuracy: 16.436464309692383, Val Loss: 1.7917730808258057, Val Accuracy: 16.645414352416992\n",
      "Iteration 190, Epoch 1, Loss: 3.578244924545288, Accuracy: 16.63121795654297, Val Loss: 1.7917776107788086, Val Accuracy: 16.645414352416992\n",
      "Iteration 200, Epoch 1, Loss: 3.489396095275879, Accuracy: 16.51896858215332, Val Loss: 1.791775107383728, Val Accuracy: 16.645414352416992\n",
      "Iteration 210, Epoch 1, Loss: 3.4089505672454834, Accuracy: 16.484004974365234, Val Loss: 1.7917711734771729, Val Accuracy: 16.645414352416992\n",
      "Iteration 220, Epoch 1, Loss: 3.335773468017578, Accuracy: 16.55118751525879, Val Loss: 1.7917706966400146, Val Accuracy: 16.645414352416992\n",
      "Iteration 230, Epoch 1, Loss: 3.2689411640167236, Accuracy: 16.524621963500977, Val Loss: 1.7917711734771729, Val Accuracy: 16.645414352416992\n",
      "Iteration 240, Epoch 1, Loss: 3.207641839981079, Accuracy: 16.591028213500977, Val Loss: 1.7917722463607788, Val Accuracy: 16.645414352416992\n",
      "Iteration 250, Epoch 1, Loss: 3.1512346267700195, Accuracy: 16.589889526367188, Val Loss: 1.791773796081543, Val Accuracy: 16.645414352416992\n",
      "Iteration 260, Epoch 1, Loss: 3.0991458892822266, Accuracy: 16.672653198242188, Val Loss: 1.7917760610580444, Val Accuracy: 16.645414352416992\n",
      "Iteration 270, Epoch 1, Loss: 3.0509157180786133, Accuracy: 16.55327606201172, Val Loss: 1.7917745113372803, Val Accuracy: 16.645414352416992\n",
      "Iteration 280, Epoch 1, Loss: 3.0061028003692627, Accuracy: 16.58696746826172, Val Loss: 1.7917747497558594, Val Accuracy: 16.645414352416992\n",
      "Iteration 290, Epoch 1, Loss: 2.9643797874450684, Accuracy: 16.543170928955078, Val Loss: 1.7917732000350952, Val Accuracy: 16.645414352416992\n",
      "Iteration 300, Epoch 1, Loss: 2.9254188537597656, Accuracy: 16.54900360107422, Val Loss: 1.7917715311050415, Val Accuracy: 16.645414352416992\n",
      "Iteration 310, Epoch 1, Loss: 2.8889682292938232, Accuracy: 16.544414520263672, Val Loss: 1.7917743921279907, Val Accuracy: 16.645414352416992\n",
      "Iteration 320, Epoch 1, Loss: 2.854779005050659, Accuracy: 16.579050064086914, Val Loss: 1.791776418685913, Val Accuracy: 16.645414352416992\n",
      "Iteration 330, Epoch 1, Loss: 2.8226664066314697, Accuracy: 16.61159324645996, Val Loss: 1.7917778491973877, Val Accuracy: 16.645414352416992\n",
      "Iteration 340, Epoch 1, Loss: 2.792426586151123, Accuracy: 16.6239013671875, Val Loss: 1.7917811870574951, Val Accuracy: 16.645414352416992\n",
      "Iteration 350, Epoch 1, Loss: 2.76391339302063, Accuracy: 16.59989356994629, Val Loss: 1.7917869091033936, Val Accuracy: 16.645414352416992\n",
      "Iteration 360, Epoch 1, Loss: 2.7369871139526367, Accuracy: 16.620498657226562, Val Loss: 1.7917884588241577, Val Accuracy: 16.645414352416992\n",
      "Iteration 370, Epoch 1, Loss: 2.7115166187286377, Accuracy: 16.589454650878906, Val Loss: 1.791788101196289, Val Accuracy: 16.674840927124023\n",
      "Iteration 380, Epoch 1, Loss: 2.6873767375946045, Accuracy: 16.543636322021484, Val Loss: 1.7917858362197876, Val Accuracy: 16.674840927124023\n",
      "Iteration 390, Epoch 1, Loss: 2.6644723415374756, Accuracy: 16.512147903442383, Val Loss: 1.791786551475525, Val Accuracy: 16.645414352416992\n",
      "Iteration 400, Epoch 1, Loss: 2.6427156925201416, Accuracy: 16.505611419677734, Val Loss: 1.7917811870574951, Val Accuracy: 16.645414352416992\n",
      "Iteration 410, Epoch 1, Loss: 2.6220154762268066, Accuracy: 16.487987518310547, Val Loss: 1.7917792797088623, Val Accuracy: 16.645414352416992\n",
      "Iteration 420, Epoch 1, Loss: 2.6022937297821045, Accuracy: 16.489755630493164, Val Loss: 1.7917782068252563, Val Accuracy: 16.645414352416992\n",
      "Iteration 430, Epoch 1, Loss: 2.583493232727051, Accuracy: 16.484193801879883, Val Loss: 1.7917754650115967, Val Accuracy: 16.645414352416992\n",
      "Iteration 440, Epoch 1, Loss: 2.5655465126037598, Accuracy: 16.528486251831055, Val Loss: 1.791771650314331, Val Accuracy: 16.645414352416992\n",
      "Iteration 450, Epoch 1, Loss: 2.5483882427215576, Accuracy: 16.508453369140625, Val Loss: 1.7917730808258057, Val Accuracy: 16.645414352416992\n",
      "Iteration 460, Epoch 1, Loss: 2.531980276107788, Accuracy: 16.506237030029297, Val Loss: 1.7917709350585938, Val Accuracy: 16.645414352416992\n",
      "Iteration 470, Epoch 1, Loss: 2.5162670612335205, Accuracy: 16.504114151000977, Val Loss: 1.7917696237564087, Val Accuracy: 16.645414352416992\n",
      "Iteration 480, Epoch 1, Loss: 2.5012052059173584, Accuracy: 16.528066635131836, Val Loss: 1.7917686700820923, Val Accuracy: 16.645414352416992\n",
      "Iteration 490, Epoch 1, Loss: 2.486758232116699, Accuracy: 16.51603889465332, Val Loss: 1.791766881942749, Val Accuracy: 16.645414352416992\n",
      "Iteration 500, Epoch 1, Loss: 2.4728856086730957, Accuracy: 16.50760841369629, Val Loss: 1.7917659282684326, Val Accuracy: 16.645414352416992\n",
      "Iteration 510, Epoch 1, Loss: 2.459557294845581, Accuracy: 16.499509811401367, Val Loss: 1.791765570640564, Val Accuracy: 16.645414352416992\n",
      "Iteration 520, Epoch 1, Loss: 2.4467430114746094, Accuracy: 16.482725143432617, Val Loss: 1.791764497756958, Val Accuracy: 16.645414352416992\n",
      "Iteration 530, Epoch 1, Loss: 2.434410810470581, Accuracy: 16.440088272094727, Val Loss: 1.7917639017105103, Val Accuracy: 16.645414352416992\n",
      "Iteration 540, Epoch 1, Loss: 2.4225311279296875, Accuracy: 16.471233367919922, Val Loss: 1.791764497756958, Val Accuracy: 16.645414352416992\n",
      "Iteration 550, Epoch 1, Loss: 2.411083698272705, Accuracy: 16.441696166992188, Val Loss: 1.7917652130126953, Val Accuracy: 16.645414352416992\n",
      "Iteration 560, Epoch 1, Loss: 2.4000446796417236, Accuracy: 16.460561752319336, Val Loss: 1.7917664051055908, Val Accuracy: 16.6650333404541\n",
      "Iteration 570, Epoch 1, Loss: 2.389392852783203, Accuracy: 16.39667320251465, Val Loss: 1.7917677164077759, Val Accuracy: 16.674840927124023\n",
      "Iteration 580, Epoch 1, Loss: 2.3791110515594482, Accuracy: 16.345739364624023, Val Loss: 1.7917661666870117, Val Accuracy: 16.674840927124023\n",
      "Iteration 590, Epoch 1, Loss: 2.3691747188568115, Accuracy: 16.35205078125, Val Loss: 1.791762351989746, Val Accuracy: 16.645414352416992\n",
      "Iteration 600, Epoch 1, Loss: 2.359567165374756, Accuracy: 16.358154296875, Val Loss: 1.7917606830596924, Val Accuracy: 16.645414352416992\n",
      "Iteration 610, Epoch 1, Loss: 2.3502748012542725, Accuracy: 16.33592414855957, Val Loss: 1.7917591333389282, Val Accuracy: 16.6650333404541\n",
      "Iteration 620, Epoch 1, Loss: 2.3412814140319824, Accuracy: 16.32699203491211, Val Loss: 1.7917587757110596, Val Accuracy: 16.645414352416992\n",
      "Iteration 630, Epoch 1, Loss: 2.3325741291046143, Accuracy: 16.328248977661133, Val Loss: 1.7917598485946655, Val Accuracy: 16.645414352416992\n",
      "Iteration 640, Epoch 1, Loss: 2.3241379261016846, Accuracy: 16.33677864074707, Val Loss: 1.7917611598968506, Val Accuracy: 16.674840927124023\n",
      "Iteration 650, Epoch 1, Loss: 2.3159613609313965, Accuracy: 16.31624412536621, Val Loss: 1.791762351989746, Val Accuracy: 16.674840927124023\n",
      "Iteration 660, Epoch 1, Loss: 2.308029890060425, Accuracy: 16.341243743896484, Val Loss: 1.7917639017105103, Val Accuracy: 16.674840927124023\n",
      "Iteration 670, Epoch 1, Loss: 2.3003363609313965, Accuracy: 16.35385513305664, Val Loss: 1.791766881942749, Val Accuracy: 16.674840927124023\n",
      "Iteration 680, Epoch 1, Loss: 2.2928686141967773, Accuracy: 16.382160186767578, Val Loss: 1.7917674779891968, Val Accuracy: 16.674840927124023\n",
      "Iteration 690, Epoch 1, Loss: 2.285616159439087, Accuracy: 16.384769439697266, Val Loss: 1.791766881942749, Val Accuracy: 16.674840927124023\n",
      "Iteration 700, Epoch 1, Loss: 2.2785723209381104, Accuracy: 16.385074615478516, Val Loss: 1.7917667627334595, Val Accuracy: 16.674840927124023\n",
      "Iteration 710, Epoch 1, Loss: 2.2717249393463135, Accuracy: 16.391965866088867, Val Loss: 1.7917652130126953, Val Accuracy: 16.674840927124023\n",
      "Iteration 720, Epoch 1, Loss: 2.265068531036377, Accuracy: 16.389995574951172, Val Loss: 1.791765570640564, Val Accuracy: 16.674840927124023\n",
      "Iteration 730, Epoch 1, Loss: 2.2585954666137695, Accuracy: 16.370981216430664, Val Loss: 1.7917640209197998, Val Accuracy: 16.674840927124023\n",
      "Iteration 740, Epoch 1, Loss: 2.2522947788238525, Accuracy: 16.384109497070312, Val Loss: 1.7917629480361938, Val Accuracy: 16.674840927124023\n",
      "Iteration 750, Epoch 1, Loss: 2.2461628913879395, Accuracy: 16.405210494995117, Val Loss: 1.7917639017105103, Val Accuracy: 16.674840927124023\n",
      "Iteration 760, Epoch 1, Loss: 2.2401959896087646, Accuracy: 16.362104415893555, Val Loss: 1.791762351989746, Val Accuracy: 16.674840927124023\n",
      "Iteration 770, Epoch 1, Loss: 2.2343757152557373, Accuracy: 16.38699722290039, Val Loss: 1.7917652130126953, Val Accuracy: 16.674840927124023\n",
      "Iteration 780, Epoch 1, Loss: 2.228708505630493, Accuracy: 16.391244888305664, Val Loss: 1.79176926612854, Val Accuracy: 16.674840927124023\n",
      "Iteration 790, Epoch 1, Loss: 2.223184585571289, Accuracy: 16.397361755371094, Val Loss: 1.7917722463607788, Val Accuracy: 16.674840927124023\n",
      "Iteration 800, Epoch 1, Loss: 2.217803716659546, Accuracy: 16.38966941833496, Val Loss: 1.7917671203613281, Val Accuracy: 16.674840927124023\n",
      "Iteration 810, Epoch 1, Loss: 2.2125513553619385, Accuracy: 16.40336036682129, Val Loss: 1.7917665243148804, Val Accuracy: 16.674840927124023\n",
      "Iteration 820, Epoch 1, Loss: 2.2074286937713623, Accuracy: 16.38817024230957, Val Loss: 1.791764497756958, Val Accuracy: 16.674840927124023\n",
      "Iteration 830, Epoch 1, Loss: 2.2024271488189697, Accuracy: 16.3808650970459, Val Loss: 1.7917654514312744, Val Accuracy: 16.674840927124023\n",
      "Iteration 840, Epoch 1, Loss: 2.1975455284118652, Accuracy: 16.36073112487793, Val Loss: 1.791764259338379, Val Accuracy: 16.6650333404541\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 6\n",
    "\n",
    "def model_init_fn():\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(3, 32, 32)),\n",
    "        tf.keras.layers.Permute((2, 3, 1)),  # NCHW -> NHWC\n",
    "        tf.keras.layers.Conv2D(channel_1, (5, 5), padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(channel_2, (3, 3), padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "    ])\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=0.9, momentum=0.0, epsilon=1e-7)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQSaJwKOUCOs"
   },
   "source": [
    "# Part IV: Keras Sequential API\n",
    "In Part III we introduced the `tf.keras.Model` API, which allows you to define models with any number of learnable layers and with arbitrary connectivity between layers.\n",
    "\n",
    "However for many models you don't need such flexibility - a lot of models can be expressed as a sequential stack of layers, with the output of each layer fed to the next layer as input. If your model fits this pattern, then there is an even easier way to define your model: using `tf.keras.Sequential`. You don't need to write any custom classes; you simply call the `tf.keras.Sequential` constructor with a list containing a sequence of layer objects.\n",
    "\n",
    "One complication with `tf.keras.Sequential` is that you must define the shape of the input to the model by passing a value to the `input_shape` of the first layer in your model.\n",
    "\n",
    "### Keras Sequential API: Two-Layer Network\n",
    "In this subsection, we will rewrite the two-layer fully-connected network using `tf.keras.Sequential`, and train it using the training loop defined above.\n",
    "\n",
    "You don't need to perform any hyperparameter tuning here, but you should see validation accuracies above 30% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 96.13736724853516, Accuracy: 18.75, Val Loss: 90.71708679199219, Val Accuracy: 18.67582130432129\n",
      "Iteration 10, Epoch 1, Loss: 85.17745208740234, Accuracy: 19.176136016845703, Val Loss: 93.24380493164062, Val Accuracy: 22.61893081665039\n",
      "Iteration 20, Epoch 1, Loss: 84.64188385009766, Accuracy: 21.428571701049805, Val Loss: 77.28785705566406, Val Accuracy: 24.100048065185547\n",
      "Iteration 30, Epoch 1, Loss: 80.92422485351562, Accuracy: 22.47983741760254, Val Loss: 91.32759857177734, Val Accuracy: 20.647377014160156\n",
      "Iteration 40, Epoch 1, Loss: 82.27729797363281, Accuracy: 22.942073822021484, Val Loss: 78.13467407226562, Val Accuracy: 24.914173126220703\n",
      "Iteration 50, Epoch 1, Loss: 81.52835083007812, Accuracy: 23.4375, Val Loss: 85.81494140625, Val Accuracy: 24.54144287109375\n",
      "Iteration 60, Epoch 1, Loss: 82.60565948486328, Accuracy: 23.38627052307129, Val Loss: 73.69322204589844, Val Accuracy: 26.483572006225586\n",
      "Iteration 70, Epoch 1, Loss: 83.5599136352539, Accuracy: 23.305458068847656, Val Loss: 77.22563171386719, Val Accuracy: 24.70819091796875\n",
      "Iteration 80, Epoch 1, Loss: 82.47554016113281, Accuracy: 23.206018447875977, Val Loss: 62.99269485473633, Val Accuracy: 26.53261375427246\n",
      "Iteration 90, Epoch 1, Loss: 82.29859161376953, Accuracy: 23.420330047607422, Val Loss: 66.45342254638672, Val Accuracy: 24.198135375976562\n",
      "Iteration 100, Epoch 1, Loss: 81.787841796875, Accuracy: 23.4375, Val Loss: 83.54438781738281, Val Accuracy: 25.56155014038086\n",
      "Iteration 110, Epoch 1, Loss: 80.60881042480469, Accuracy: 23.620494842529297, Val Loss: 55.73799514770508, Val Accuracy: 28.886709213256836\n",
      "Iteration 120, Epoch 1, Loss: 79.5162582397461, Accuracy: 23.82489776611328, Val Loss: 55.53093338012695, Val Accuracy: 29.56351089477539\n",
      "Iteration 130, Epoch 1, Loss: 78.2486343383789, Accuracy: 24.12929344177246, Val Loss: 60.5871696472168, Val Accuracy: 28.9455623626709\n",
      "Iteration 140, Epoch 1, Loss: 76.97040557861328, Accuracy: 24.545656204223633, Val Loss: 54.413597106933594, Val Accuracy: 30.377635955810547\n",
      "Iteration 150, Epoch 1, Loss: 77.11262512207031, Accuracy: 24.441225051879883, Val Loss: 71.8921890258789, Val Accuracy: 23.256498336791992\n",
      "Iteration 160, Epoch 1, Loss: 76.44742584228516, Accuracy: 24.543867111206055, Val Loss: 74.16716766357422, Val Accuracy: 27.54291343688965\n",
      "Iteration 170, Epoch 1, Loss: 76.13666534423828, Accuracy: 24.68018913269043, Val Loss: 80.02558898925781, Val Accuracy: 27.797941207885742\n",
      "Iteration 180, Epoch 1, Loss: 75.99544525146484, Accuracy: 24.680593490600586, Val Loss: 77.92840576171875, Val Accuracy: 26.8072566986084\n",
      "Iteration 190, Epoch 1, Loss: 75.45662689208984, Accuracy: 24.76276206970215, Val Loss: 51.70720672607422, Val Accuracy: 27.13094711303711\n",
      "Iteration 200, Epoch 1, Loss: 75.47277069091797, Accuracy: 24.73569679260254, Val Loss: 63.56413650512695, Val Accuracy: 30.583620071411133\n",
      "Iteration 210, Epoch 1, Loss: 75.02781677246094, Accuracy: 24.77043914794922, Val Loss: 57.26267623901367, Val Accuracy: 28.052967071533203\n",
      "Iteration 220, Epoch 1, Loss: 74.67091369628906, Accuracy: 24.964649200439453, Val Loss: 71.3584213256836, Val Accuracy: 28.631681442260742\n",
      "Iteration 230, Epoch 1, Loss: 74.74976348876953, Accuracy: 25.05411148071289, Val Loss: 78.53874206542969, Val Accuracy: 28.62187385559082\n",
      "Iteration 240, Epoch 1, Loss: 74.18477630615234, Accuracy: 25.175052642822266, Val Loss: 57.876800537109375, Val Accuracy: 26.738595962524414\n",
      "Iteration 250, Epoch 1, Loss: 73.81831359863281, Accuracy: 25.32370376586914, Val Loss: 76.7044906616211, Val Accuracy: 27.837175369262695\n",
      "Iteration 260, Epoch 1, Loss: 73.58363342285156, Accuracy: 25.383142471313477, Val Loss: 49.597007751464844, Val Accuracy: 31.093671798706055\n",
      "Iteration 270, Epoch 1, Loss: 72.67578887939453, Accuracy: 25.639989852905273, Val Loss: 50.90686798095703, Val Accuracy: 30.730751037597656\n",
      "Iteration 280, Epoch 1, Loss: 72.11357879638672, Accuracy: 25.683942794799805, Val Loss: 65.69915771484375, Val Accuracy: 29.288867950439453\n",
      "Iteration 290, Epoch 1, Loss: 71.70918273925781, Accuracy: 25.84836769104004, Val Loss: 51.40381622314453, Val Accuracy: 27.425209045410156\n",
      "Iteration 300, Epoch 1, Loss: 71.39373016357422, Accuracy: 25.887664794921875, Val Loss: 71.07123565673828, Val Accuracy: 23.079940795898438\n",
      "Iteration 310, Epoch 1, Loss: 71.1392593383789, Accuracy: 25.919414520263672, Val Loss: 60.544464111328125, Val Accuracy: 27.346738815307617\n",
      "Iteration 320, Epoch 1, Loss: 70.74471282958984, Accuracy: 25.949182510375977, Val Loss: 71.25373840332031, Val Accuracy: 27.503677368164062\n",
      "Iteration 330, Epoch 1, Loss: 70.56395721435547, Accuracy: 25.93466567993164, Val Loss: 63.38672637939453, Val Accuracy: 27.385974884033203\n",
      "Iteration 340, Epoch 1, Loss: 70.39332580566406, Accuracy: 25.966825485229492, Val Loss: 65.13407897949219, Val Accuracy: 25.747915267944336\n",
      "Iteration 350, Epoch 1, Loss: 69.9795913696289, Accuracy: 26.0105037689209, Val Loss: 47.27614212036133, Val Accuracy: 30.02452278137207\n",
      "Iteration 360, Epoch 1, Loss: 69.73994445800781, Accuracy: 26.00848388671875, Val Loss: 59.64890670776367, Val Accuracy: 29.573322296142578\n",
      "Iteration 370, Epoch 1, Loss: 69.27327728271484, Accuracy: 26.082378387451172, Val Loss: 49.881980895996094, Val Accuracy: 30.4168701171875\n",
      "Iteration 380, Epoch 1, Loss: 69.0289306640625, Accuracy: 26.168800354003906, Val Loss: 55.59254837036133, Val Accuracy: 30.318784713745117\n",
      "Iteration 390, Epoch 1, Loss: 68.75228118896484, Accuracy: 26.270780563354492, Val Loss: 48.98808670043945, Val Accuracy: 30.82883644104004\n",
      "Iteration 400, Epoch 1, Loss: 68.38404083251953, Accuracy: 26.344295501708984, Val Loss: 53.13196563720703, Val Accuracy: 30.181461334228516\n",
      "Iteration 410, Epoch 1, Loss: 68.22352600097656, Accuracy: 26.45224952697754, Val Loss: 81.91960144042969, Val Accuracy: 23.482099533081055\n",
      "Iteration 420, Epoch 1, Loss: 67.95181274414062, Accuracy: 26.510540008544922, Val Loss: 39.132232666015625, Val Accuracy: 30.32859230041504\n",
      "Iteration 430, Epoch 1, Loss: 67.6070785522461, Accuracy: 26.551624298095703, Val Loss: 53.50677490234375, Val Accuracy: 31.083864212036133\n",
      "Iteration 440, Epoch 1, Loss: 67.1292953491211, Accuracy: 26.73611068725586, Val Loss: 54.847381591796875, Val Accuracy: 29.347721099853516\n",
      "Iteration 450, Epoch 1, Loss: 67.12129974365234, Accuracy: 26.72186851501465, Val Loss: 80.382568359375, Val Accuracy: 25.836193084716797\n",
      "Iteration 460, Epoch 1, Loss: 67.01705932617188, Accuracy: 26.769250869750977, Val Loss: 53.01361846923828, Val Accuracy: 28.896516799926758\n",
      "Iteration 470, Epoch 1, Loss: 66.96340942382812, Accuracy: 26.804670333862305, Val Loss: 48.842708587646484, Val Accuracy: 31.250614166259766\n",
      "Iteration 480, Epoch 1, Loss: 66.6385498046875, Accuracy: 26.890594482421875, Val Loss: 43.400146484375, Val Accuracy: 31.947031021118164\n",
      "Iteration 490, Epoch 1, Loss: 66.3564453125, Accuracy: 26.918914794921875, Val Loss: 48.35566711425781, Val Accuracy: 32.58460235595703\n",
      "Iteration 500, Epoch 1, Loss: 66.1065902709961, Accuracy: 26.918039321899414, Val Loss: 66.69760131835938, Val Accuracy: 28.749385833740234\n",
      "Iteration 510, Epoch 1, Loss: 65.83602142333984, Accuracy: 26.95694923400879, Val Loss: 56.457237243652344, Val Accuracy: 29.269248962402344\n",
      "Iteration 520, Epoch 1, Loss: 65.53197479248047, Accuracy: 27.033349990844727, Val Loss: 42.462371826171875, Val Accuracy: 30.10299301147461\n",
      "Iteration 530, Epoch 1, Loss: 65.40813446044922, Accuracy: 27.071561813354492, Val Loss: 58.45856857299805, Val Accuracy: 31.191761016845703\n",
      "Iteration 540, Epoch 1, Loss: 65.1026611328125, Accuracy: 27.093923568725586, Val Loss: 51.342369079589844, Val Accuracy: 30.52476692199707\n",
      "Iteration 550, Epoch 1, Loss: 64.81645202636719, Accuracy: 27.15800666809082, Val Loss: 49.2563591003418, Val Accuracy: 28.92594337463379\n",
      "Iteration 560, Epoch 1, Loss: 64.5272445678711, Accuracy: 27.21980857849121, Val Loss: 43.15010452270508, Val Accuracy: 31.888181686401367\n",
      "Iteration 570, Epoch 1, Loss: 64.40275573730469, Accuracy: 27.2712345123291, Val Loss: 79.78150939941406, Val Accuracy: 24.551250457763672\n",
      "Iteration 580, Epoch 1, Loss: 64.36937713623047, Accuracy: 27.285930633544922, Val Loss: 47.67082977294922, Val Accuracy: 33.124080657958984\n",
      "Iteration 590, Epoch 1, Loss: 64.173828125, Accuracy: 27.33978271484375, Val Loss: 51.09161376953125, Val Accuracy: 28.719961166381836\n",
      "Iteration 600, Epoch 1, Loss: 63.85404586791992, Accuracy: 27.412647247314453, Val Loss: 66.87032318115234, Val Accuracy: 28.327611923217773\n",
      "Iteration 610, Epoch 1, Loss: 63.75098419189453, Accuracy: 27.429420471191406, Val Loss: 63.25078582763672, Val Accuracy: 26.238351821899414\n",
      "Iteration 620, Epoch 1, Loss: 63.533206939697266, Accuracy: 27.463266372680664, Val Loss: 58.275733947753906, Val Accuracy: 32.1922492980957\n",
      "Iteration 630, Epoch 1, Loss: 63.35076904296875, Accuracy: 27.503467559814453, Val Loss: 39.986690521240234, Val Accuracy: 32.12358856201172\n",
      "Iteration 640, Epoch 1, Loss: 63.248817443847656, Accuracy: 27.532663345336914, Val Loss: 53.88616180419922, Val Accuracy: 31.152524948120117\n",
      "Iteration 650, Epoch 1, Loss: 63.11585998535156, Accuracy: 27.560964584350586, Val Loss: 61.32991409301758, Val Accuracy: 30.46591567993164\n",
      "Iteration 660, Epoch 1, Loss: 63.02702331542969, Accuracy: 27.614408493041992, Val Loss: 38.782814025878906, Val Accuracy: 33.369300842285156\n",
      "Iteration 670, Epoch 1, Loss: 62.83491897583008, Accuracy: 27.66859245300293, Val Loss: 45.68375015258789, Val Accuracy: 33.35948944091797\n",
      "Iteration 680, Epoch 1, Loss: 62.5933952331543, Accuracy: 27.707416534423828, Val Loss: 42.912445068359375, Val Accuracy: 33.73221969604492\n",
      "Iteration 690, Epoch 1, Loss: 62.52187728881836, Accuracy: 27.71571922302246, Val Loss: 49.84137725830078, Val Accuracy: 32.45708465576172\n",
      "Iteration 700, Epoch 1, Loss: 62.28583526611328, Accuracy: 27.777280807495117, Val Loss: 47.76424789428711, Val Accuracy: 32.084354400634766\n",
      "Iteration 710, Epoch 1, Loss: 62.082515716552734, Accuracy: 27.850299835205078, Val Loss: 41.41427993774414, Val Accuracy: 29.4458065032959\n",
      "Iteration 720, Epoch 1, Loss: 61.83015823364258, Accuracy: 27.932125091552734, Val Loss: 55.45949172973633, Val Accuracy: 30.642471313476562\n",
      "Iteration 730, Epoch 1, Loss: 61.66531753540039, Accuracy: 27.95400047302246, Val Loss: 45.496620178222656, Val Accuracy: 28.92594337463379\n",
      "Iteration 740, Epoch 1, Loss: 61.67325973510742, Accuracy: 27.8782901763916, Val Loss: 39.43711471557617, Val Accuracy: 32.13339614868164\n",
      "Iteration 750, Epoch 1, Loss: 61.439815521240234, Accuracy: 27.894058227539062, Val Loss: 60.70427703857422, Val Accuracy: 27.47425079345703\n",
      "Iteration 760, Epoch 1, Loss: 61.31663131713867, Accuracy: 27.92583656311035, Val Loss: 54.36182403564453, Val Accuracy: 30.122608184814453\n",
      "Iteration 770, Epoch 1, Loss: 61.12547302246094, Accuracy: 27.9567928314209, Val Loss: 44.530338287353516, Val Accuracy: 30.044137954711914\n",
      "Iteration 780, Epoch 1, Loss: 60.92612075805664, Accuracy: 27.986955642700195, Val Loss: 40.26387405395508, Val Accuracy: 32.39823532104492\n",
      "Iteration 790, Epoch 1, Loss: 60.67811584472656, Accuracy: 28.05388832092285, Val Loss: 62.53764724731445, Val Accuracy: 28.739578247070312\n",
      "Iteration 800, Epoch 1, Loss: 60.567138671875, Accuracy: 28.101591110229492, Val Loss: 44.313209533691406, Val Accuracy: 32.73173141479492\n",
      "Iteration 810, Epoch 1, Loss: 60.37269592285156, Accuracy: 28.1423397064209, Val Loss: 36.68394470214844, Val Accuracy: 32.97694778442383\n",
      "Iteration 820, Epoch 1, Loss: 60.26899337768555, Accuracy: 28.1763858795166, Val Loss: 48.00194549560547, Val Accuracy: 31.338891983032227\n",
      "Iteration 830, Epoch 1, Loss: 60.13775634765625, Accuracy: 28.19080924987793, Val Loss: 41.87317657470703, Val Accuracy: 31.191761016845703\n",
      "Iteration 840, Epoch 1, Loss: 60.03034973144531, Accuracy: 28.188169479370117, Val Loss: 49.435813903808594, Val Accuracy: 27.523296356201172\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 6\n",
    "learning_rate = 1e-4\n",
    "\n",
    "def model_init_fn():\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(hidden_size, activation='relu', kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer),\n",
    "    ])\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "zRnmvK6QUCOs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 65.14974212646484, Accuracy: 14.0625, Val Loss: 132.94993591308594, Val Accuracy: 18.048063278198242\n",
      "Iteration 10, Epoch 1, Loss: 30.146808624267578, Accuracy: 18.892045974731445, Val Loss: 5.266853332519531, Val Accuracy: 18.116724014282227\n",
      "Iteration 20, Epoch 1, Loss: 17.6715030670166, Accuracy: 19.494047164916992, Val Loss: 3.415821075439453, Val Accuracy: 17.861696243286133\n",
      "Iteration 30, Epoch 1, Loss: 12.948808670043945, Accuracy: 18.245967864990234, Val Loss: 2.753894329071045, Val Accuracy: 17.694948196411133\n",
      "Iteration 40, Epoch 1, Loss: 10.414810180664062, Accuracy: 18.635671615600586, Val Loss: 2.472862720489502, Val Accuracy: 18.136341094970703\n",
      "Iteration 50, Epoch 1, Loss: 8.842007637023926, Accuracy: 17.86151885986328, Val Loss: 2.270205497741699, Val Accuracy: 18.048063278198242\n",
      "Iteration 60, Epoch 1, Loss: 7.750801086425781, Accuracy: 17.776639938354492, Val Loss: 2.1748085021972656, Val Accuracy: 17.685138702392578\n",
      "Iteration 70, Epoch 1, Loss: 6.9670867919921875, Accuracy: 17.891725540161133, Val Loss: 2.1020350456237793, Val Accuracy: 17.6655216217041\n",
      "Iteration 80, Epoch 1, Loss: 6.361310005187988, Accuracy: 18.094135284423828, Val Loss: 2.0504536628723145, Val Accuracy: 18.106914520263672\n",
      "Iteration 90, Epoch 1, Loss: 5.881819725036621, Accuracy: 18.028846740722656, Val Loss: 2.017451763153076, Val Accuracy: 18.283472061157227\n",
      "Iteration 100, Epoch 1, Loss: 5.497776031494141, Accuracy: 17.991954803466797, Val Loss: 1.9859883785247803, Val Accuracy: 18.381559371948242\n",
      "Iteration 110, Epoch 1, Loss: 5.17988395690918, Accuracy: 17.863174438476562, Val Loss: 1.9603124856948853, Val Accuracy: 18.59735107421875\n",
      "Iteration 120, Epoch 1, Loss: 4.910791873931885, Accuracy: 17.910640716552734, Val Loss: 1.9448826313018799, Val Accuracy: 18.312898635864258\n",
      "Iteration 130, Epoch 1, Loss: 4.677631378173828, Accuracy: 17.95085906982422, Val Loss: 1.9320074319839478, Val Accuracy: 18.577733993530273\n",
      "Iteration 140, Epoch 1, Loss: 4.482027053833008, Accuracy: 18.140514373779297, Val Loss: 1.9194469451904297, Val Accuracy: 18.999509811401367\n",
      "Iteration 150, Epoch 1, Loss: 4.313668727874756, Accuracy: 18.09809684753418, Val Loss: 1.9065313339233398, Val Accuracy: 18.773908615112305\n",
      "Iteration 160, Epoch 1, Loss: 4.164186000823975, Accuracy: 18.109472274780273, Val Loss: 1.8953930139541626, Val Accuracy: 18.783716201782227\n",
      "Iteration 170, Epoch 1, Loss: 4.029619216918945, Accuracy: 18.30226707458496, Val Loss: 1.8883533477783203, Val Accuracy: 18.803333282470703\n",
      "Iteration 180, Epoch 1, Loss: 3.912179946899414, Accuracy: 18.249309539794922, Val Loss: 1.878283143043518, Val Accuracy: 19.048553466796875\n",
      "Iteration 190, Epoch 1, Loss: 3.806462526321411, Accuracy: 18.095550537109375, Val Loss: 1.872399926185608, Val Accuracy: 18.940656661987305\n",
      "Iteration 200, Epoch 1, Loss: 3.7100799083709717, Accuracy: 18.198070526123047, Val Loss: 1.8656057119369507, Val Accuracy: 19.20549201965332\n",
      "Iteration 210, Epoch 1, Loss: 3.6204357147216797, Accuracy: 18.283472061157227, Val Loss: 1.861588716506958, Val Accuracy: 19.372241973876953\n",
      "Iteration 220, Epoch 1, Loss: 3.5390169620513916, Accuracy: 18.332860946655273, Val Loss: 1.8580350875854492, Val Accuracy: 19.509563446044922\n",
      "Iteration 230, Epoch 1, Loss: 3.464587926864624, Accuracy: 18.51325798034668, Val Loss: 1.8510923385620117, Val Accuracy: 19.489946365356445\n",
      "Iteration 240, Epoch 1, Loss: 3.3979315757751465, Accuracy: 18.445280075073242, Val Loss: 1.8458648920059204, Val Accuracy: 19.607650756835938\n",
      "Iteration 250, Epoch 1, Loss: 3.3356142044067383, Accuracy: 18.46986961364746, Val Loss: 1.8424370288848877, Val Accuracy: 19.852869033813477\n",
      "Iteration 260, Epoch 1, Loss: 3.2784671783447266, Accuracy: 18.444684982299805, Val Loss: 1.8394838571548462, Val Accuracy: 19.794017791748047\n",
      "Iteration 270, Epoch 1, Loss: 3.224020481109619, Accuracy: 18.588560104370117, Val Loss: 1.8369405269622803, Val Accuracy: 20.009807586669922\n",
      "Iteration 280, Epoch 1, Loss: 3.1743462085723877, Accuracy: 18.572063446044922, Val Loss: 1.8344192504882812, Val Accuracy: 19.901912689208984\n",
      "Iteration 290, Epoch 1, Loss: 3.1292550563812256, Accuracy: 18.621135711669922, Val Loss: 1.8321870565414429, Val Accuracy: 19.607650756835938\n",
      "Iteration 300, Epoch 1, Loss: 3.0872585773468018, Accuracy: 18.594268798828125, Val Loss: 1.829245924949646, Val Accuracy: 19.72535514831543\n",
      "Iteration 310, Epoch 1, Loss: 3.0464351177215576, Accuracy: 18.619373321533203, Val Loss: 1.8289045095443726, Val Accuracy: 19.48013687133789\n",
      "Iteration 320, Epoch 1, Loss: 3.008596897125244, Accuracy: 18.623443603515625, Val Loss: 1.824371576309204, Val Accuracy: 19.823442459106445\n",
      "Iteration 330, Epoch 1, Loss: 2.9736263751983643, Accuracy: 18.650869369506836, Val Loss: 1.8223127126693726, Val Accuracy: 19.63707733154297\n",
      "Iteration 340, Epoch 1, Loss: 2.9405720233917236, Accuracy: 18.70876121520996, Val Loss: 1.820896863937378, Val Accuracy: 19.460519790649414\n",
      "Iteration 350, Epoch 1, Loss: 2.909019947052002, Accuracy: 18.683225631713867, Val Loss: 1.8181533813476562, Val Accuracy: 19.411476135253906\n",
      "Iteration 360, Epoch 1, Loss: 2.879142999649048, Accuracy: 18.676420211791992, Val Loss: 1.8165454864501953, Val Accuracy: 19.470327377319336\n",
      "Iteration 370, Epoch 1, Loss: 2.8502109050750732, Accuracy: 18.644710540771484, Val Loss: 1.8149378299713135, Val Accuracy: 19.627267837524414\n",
      "Iteration 380, Epoch 1, Loss: 2.8233354091644287, Accuracy: 18.581857681274414, Val Loss: 1.8133147954940796, Val Accuracy: 19.676311492919922\n",
      "Iteration 390, Epoch 1, Loss: 2.7968759536743164, Accuracy: 18.710037231445312, Val Loss: 1.8123843669891357, Val Accuracy: 19.735164642333984\n",
      "Iteration 400, Epoch 1, Loss: 2.772402763366699, Accuracy: 18.718828201293945, Val Loss: 1.8106740713119507, Val Accuracy: 19.607650756835938\n",
      "Iteration 410, Epoch 1, Loss: 2.7487611770629883, Accuracy: 18.71198272705078, Val Loss: 1.8096342086791992, Val Accuracy: 19.627267837524414\n",
      "Iteration 420, Epoch 1, Loss: 2.726085901260376, Accuracy: 18.73886489868164, Val Loss: 1.8091224431991577, Val Accuracy: 19.744972229003906\n",
      "Iteration 430, Epoch 1, Loss: 2.704216480255127, Accuracy: 18.742748260498047, Val Loss: 1.8081518411636353, Val Accuracy: 19.58803367614746\n",
      "Iteration 440, Epoch 1, Loss: 2.6836888790130615, Accuracy: 18.785430908203125, Val Loss: 1.8073804378509521, Val Accuracy: 19.744972229003906\n",
      "Iteration 450, Epoch 1, Loss: 2.664212465286255, Accuracy: 18.812360763549805, Val Loss: 1.8075191974639893, Val Accuracy: 19.499755859375\n",
      "Iteration 460, Epoch 1, Loss: 2.6451973915100098, Accuracy: 18.87201690673828, Val Loss: 1.806604027748108, Val Accuracy: 20.02942657470703\n",
      "Iteration 470, Epoch 1, Loss: 2.626944065093994, Accuracy: 18.869426727294922, Val Loss: 1.8071340322494507, Val Accuracy: 19.990190505981445\n",
      "Iteration 480, Epoch 1, Loss: 2.6098718643188477, Accuracy: 18.86369514465332, Val Loss: 1.8045759201049805, Val Accuracy: 19.91172218322754\n",
      "Iteration 490, Epoch 1, Loss: 2.5933077335357666, Accuracy: 18.925025939941406, Val Loss: 1.8042923212051392, Val Accuracy: 19.794017791748047\n",
      "Iteration 500, Epoch 1, Loss: 2.5781607627868652, Accuracy: 18.887226104736328, Val Loss: 1.803615927696228, Val Accuracy: 19.705738067626953\n",
      "Iteration 510, Epoch 1, Loss: 2.5626792907714844, Accuracy: 18.921232223510742, Val Loss: 1.8019282817840576, Val Accuracy: 19.607650756835938\n",
      "Iteration 520, Epoch 1, Loss: 2.547881841659546, Accuracy: 18.965930938720703, Val Loss: 1.80169677734375, Val Accuracy: 19.823442459106445\n",
      "Iteration 530, Epoch 1, Loss: 2.533463478088379, Accuracy: 19.00894546508789, Val Loss: 1.802228569984436, Val Accuracy: 19.80382537841797\n",
      "Iteration 540, Epoch 1, Loss: 2.519993305206299, Accuracy: 19.03592872619629, Val Loss: 1.799959421157837, Val Accuracy: 19.509563446044922\n",
      "Iteration 550, Epoch 1, Loss: 2.507094144821167, Accuracy: 19.010889053344727, Val Loss: 1.800737977027893, Val Accuracy: 19.5291805267334\n",
      "Iteration 560, Epoch 1, Loss: 2.4948370456695557, Accuracy: 19.05080223083496, Val Loss: 1.7995742559432983, Val Accuracy: 19.45071029663086\n",
      "Iteration 570, Epoch 1, Loss: 2.482860803604126, Accuracy: 19.023643493652344, Val Loss: 1.7974462509155273, Val Accuracy: 19.578224182128906\n",
      "Iteration 580, Epoch 1, Loss: 2.4711551666259766, Accuracy: 19.064651489257812, Val Loss: 1.7960361242294312, Val Accuracy: 19.735164642333984\n",
      "Iteration 590, Epoch 1, Loss: 2.4591031074523926, Accuracy: 19.11220359802246, Val Loss: 1.7962614297866821, Val Accuracy: 19.744972229003906\n",
      "Iteration 600, Epoch 1, Loss: 2.4480786323547363, Accuracy: 19.17377281188965, Val Loss: 1.7964032888412476, Val Accuracy: 19.833251953125\n",
      "Iteration 610, Epoch 1, Loss: 2.437370777130127, Accuracy: 19.148937225341797, Val Loss: 1.7944393157958984, Val Accuracy: 19.705738067626953\n",
      "Iteration 620, Epoch 1, Loss: 2.4270341396331787, Accuracy: 19.137479782104492, Val Loss: 1.794226050376892, Val Accuracy: 19.63707733154297\n",
      "Iteration 630, Epoch 1, Loss: 2.417224168777466, Accuracy: 19.09667205810547, Val Loss: 1.7945915460586548, Val Accuracy: 19.509563446044922\n",
      "Iteration 640, Epoch 1, Loss: 2.4072823524475098, Accuracy: 19.142454147338867, Val Loss: 1.7930879592895508, Val Accuracy: 19.55860710144043\n",
      "Iteration 650, Epoch 1, Loss: 2.397697925567627, Accuracy: 19.131624221801758, Val Loss: 1.792123794555664, Val Accuracy: 19.656694412231445\n",
      "Iteration 660, Epoch 1, Loss: 2.3883886337280273, Accuracy: 19.1778564453125, Val Loss: 1.7917165756225586, Val Accuracy: 19.735164642333984\n",
      "Iteration 670, Epoch 1, Loss: 2.3794126510620117, Accuracy: 19.19942283630371, Val Loss: 1.7913081645965576, Val Accuracy: 19.774398803710938\n",
      "Iteration 680, Epoch 1, Loss: 2.3707141876220703, Accuracy: 19.245594024658203, Val Loss: 1.7907196283340454, Val Accuracy: 19.8626766204834\n",
      "Iteration 690, Epoch 1, Loss: 2.3618199825286865, Accuracy: 19.25425148010254, Val Loss: 1.7906596660614014, Val Accuracy: 19.990190505981445\n",
      "Iteration 700, Epoch 1, Loss: 2.3535971641540527, Accuracy: 19.26266098022461, Val Loss: 1.7912601232528687, Val Accuracy: 19.715547561645508\n",
      "Iteration 710, Epoch 1, Loss: 2.3455092906951904, Accuracy: 19.290611267089844, Val Loss: 1.7898114919662476, Val Accuracy: 19.833251953125\n",
      "Iteration 720, Epoch 1, Loss: 2.3376059532165527, Accuracy: 19.313453674316406, Val Loss: 1.789588212966919, Val Accuracy: 19.784208297729492\n",
      "Iteration 730, Epoch 1, Loss: 2.330129623413086, Accuracy: 19.290782928466797, Val Loss: 1.7893297672271729, Val Accuracy: 20.009807586669922\n",
      "Iteration 740, Epoch 1, Loss: 2.3227343559265137, Accuracy: 19.319332122802734, Val Loss: 1.788800597190857, Val Accuracy: 19.990190505981445\n",
      "Iteration 750, Epoch 1, Loss: 2.3156495094299316, Accuracy: 19.30759048461914, Val Loss: 1.7887287139892578, Val Accuracy: 20.10789680480957\n",
      "Iteration 760, Epoch 1, Loss: 2.3086888790130615, Accuracy: 19.351593017578125, Val Loss: 1.7883694171905518, Val Accuracy: 20.009807586669922\n",
      "Iteration 770, Epoch 1, Loss: 2.30212664604187, Accuracy: 19.347843170166016, Val Loss: 1.788187026977539, Val Accuracy: 19.91172218322754\n",
      "Iteration 780, Epoch 1, Loss: 2.2955782413482666, Accuracy: 19.32218360900879, Val Loss: 1.7876983880996704, Val Accuracy: 19.97057342529297\n",
      "Iteration 790, Epoch 1, Loss: 2.2891061305999756, Accuracy: 19.318899154663086, Val Loss: 1.7871761322021484, Val Accuracy: 20.039234161376953\n",
      "Iteration 800, Epoch 1, Loss: 2.2827837467193604, Accuracy: 19.36251449584961, Val Loss: 1.7865241765975952, Val Accuracy: 19.960765838623047\n",
      "Iteration 810, Epoch 1, Loss: 2.2764976024627686, Accuracy: 19.391569137573242, Val Loss: 1.7869898080825806, Val Accuracy: 20.009807586669922\n",
      "Iteration 820, Epoch 1, Loss: 2.270393133163452, Accuracy: 19.39327049255371, Val Loss: 1.7867224216461182, Val Accuracy: 20.039234161376953\n",
      "Iteration 830, Epoch 1, Loss: 2.2645113468170166, Accuracy: 19.396812438964844, Val Loss: 1.7859752178192139, Val Accuracy: 19.843059539794922\n",
      "Iteration 840, Epoch 1, Loss: 2.2588236331939697, Accuracy: 19.38912010192871, Val Loss: 1.7856496572494507, Val Accuracy: 19.921531677246094\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 6\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes) # Now I use the ThreeLayerConvNet defined above again\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate) # Vanilla SGD optimizer\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWYc9fWCUCOu"
   },
   "source": [
    "### Abstracting Away the Training Loop\n",
    "In the previous examples, we used a customised training loop to train models (e.g. `train_part34`). Writing your own training loop is only required if you need more flexibility and control during training your model. Alternately, you can also use  built-in APIs like `tf.keras.Model.fit()` and `tf.keras.Model.evaluate` to train and evaluate a model. Also remember to configure your model for training by calling `tf.keras.Model.compile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "6N9Ue-KFUCOu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 17s 20ms/step - loss: 7.3862 - sparse_categorical_accuracy: 0.1866 - val_loss: 1.7934 - val_sparse_categorical_accuracy: 0.1929\n",
      "319/319 [==============================] - 1s 4ms/step - loss: 1.7927 - sparse_categorical_accuracy: 0.1940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.792702078819275, 0.19399765133857727]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWQChkcAUCOw"
   },
   "source": [
    "### Keras Sequential API: Three-Layer ConvNet\n",
    "Here you should use `tf.keras.Sequential` to reimplement the same three-layer ConvNet architecture used in Part II and Part III. As a reminder, your model should have the following architecture:\n",
    "\n",
    "1. Convolutional layer with 32 5x5 kernels, using zero padding of 2\n",
    "2. ReLU nonlinearity\n",
    "3. Convolutional layer with 16 3x3 kernels, using zero padding of 1\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer giving class scores\n",
    "6. Softmax nonlinearity\n",
    "\n",
    "You should initialize the weights of the model using a `tf.initializers.VarianceScaling` as above.\n",
    "\n",
    "You should train the model using Nesterov momentum 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 146.49871826171875, Accuracy: 12.5, Val Loss: 2006.504638671875, Val Accuracy: 16.6650333404541\n",
      "Iteration 10, Epoch 1, Loss: 179.66001892089844, Accuracy: 15.909090995788574, Val Loss: 1.7917280197143555, Val Accuracy: 16.733692169189453\n",
      "Iteration 20, Epoch 1, Loss: 94.96086120605469, Accuracy: 16.29464340209961, Val Loss: 1.7917730808258057, Val Accuracy: 16.674840927124023\n",
      "Iteration 30, Epoch 1, Loss: 64.9062728881836, Accuracy: 16.481855392456055, Val Loss: 1.7917661666870117, Val Accuracy: 16.674840927124023\n",
      "Iteration 40, Epoch 1, Loss: 49.51245880126953, Accuracy: 17.03506088256836, Val Loss: 1.79176926612854, Val Accuracy: 16.674840927124023\n",
      "Iteration 50, Epoch 1, Loss: 40.155452728271484, Accuracy: 16.973039627075195, Val Loss: 1.7917711734771729, Val Accuracy: 16.674840927124023\n",
      "Iteration 60, Epoch 1, Loss: 33.866329193115234, Accuracy: 16.880123138427734, Val Loss: 1.7917706966400146, Val Accuracy: 16.674840927124023\n",
      "Iteration 70, Epoch 1, Loss: 29.348804473876953, Accuracy: 16.967430114746094, Val Loss: 1.7917702198028564, Val Accuracy: 16.674840927124023\n",
      "Iteration 80, Epoch 1, Loss: 25.946683883666992, Accuracy: 16.956018447875977, Val Loss: 1.791770577430725, Val Accuracy: 16.674840927124023\n",
      "Iteration 90, Epoch 1, Loss: 23.292322158813477, Accuracy: 16.638050079345703, Val Loss: 1.7917696237564087, Val Accuracy: 16.674840927124023\n",
      "Iteration 100, Epoch 1, Loss: 21.16351318359375, Accuracy: 16.7852725982666, Val Loss: 1.7917730808258057, Val Accuracy: 16.674840927124023\n",
      "Iteration 110, Epoch 1, Loss: 19.418325424194336, Accuracy: 16.554054260253906, Val Loss: 1.7917730808258057, Val Accuracy: 16.674840927124023\n",
      "Iteration 120, Epoch 1, Loss: 17.96159553527832, Accuracy: 16.658058166503906, Val Loss: 1.791774034500122, Val Accuracy: 16.674840927124023\n",
      "Iteration 130, Epoch 1, Loss: 16.72728157043457, Accuracy: 16.579198837280273, Val Loss: 1.791770577430725, Val Accuracy: 16.645414352416992\n",
      "Iteration 140, Epoch 1, Loss: 15.668041229248047, Accuracy: 16.566932678222656, Val Loss: 1.7917671203613281, Val Accuracy: 16.645414352416992\n",
      "Iteration 150, Epoch 1, Loss: 14.749068260192871, Accuracy: 16.57698631286621, Val Loss: 1.7917664051055908, Val Accuracy: 16.645414352416992\n",
      "Iteration 160, Epoch 1, Loss: 13.944255828857422, Accuracy: 16.546972274780273, Val Loss: 1.7917683124542236, Val Accuracy: 16.645414352416992\n",
      "Iteration 170, Epoch 1, Loss: 13.233601570129395, Accuracy: 16.584428787231445, Val Loss: 1.7917696237564087, Val Accuracy: 16.645414352416992\n",
      "Iteration 180, Epoch 1, Loss: 12.60146427154541, Accuracy: 16.540054321289062, Val Loss: 1.7917678356170654, Val Accuracy: 16.645414352416992\n",
      "Iteration 190, Epoch 1, Loss: 12.035505294799805, Accuracy: 16.729385375976562, Val Loss: 1.7917721271514893, Val Accuracy: 16.645414352416992\n",
      "Iteration 200, Epoch 1, Loss: 11.525887489318848, Accuracy: 16.61225128173828, Val Loss: 1.7917718887329102, Val Accuracy: 16.645414352416992\n",
      "Iteration 210, Epoch 1, Loss: 11.064560890197754, Accuracy: 16.572866439819336, Val Loss: 1.79176926612854, Val Accuracy: 16.645414352416992\n",
      "Iteration 220, Epoch 1, Loss: 10.644978523254395, Accuracy: 16.636028289794922, Val Loss: 1.79176926612854, Val Accuracy: 16.645414352416992\n",
      "Iteration 230, Epoch 1, Loss: 10.261730194091797, Accuracy: 16.605789184570312, Val Loss: 1.79176926612854, Val Accuracy: 16.645414352416992\n",
      "Iteration 240, Epoch 1, Loss: 9.910273551940918, Accuracy: 16.668827056884766, Val Loss: 1.7917712926864624, Val Accuracy: 16.645414352416992\n",
      "Iteration 250, Epoch 1, Loss: 9.586836814880371, Accuracy: 16.66459083557129, Val Loss: 1.7917722463607788, Val Accuracy: 16.645414352416992\n",
      "Iteration 260, Epoch 1, Loss: 9.288168907165527, Accuracy: 16.74449348449707, Val Loss: 1.7917730808258057, Val Accuracy: 16.645414352416992\n",
      "Iteration 270, Epoch 1, Loss: 9.011558532714844, Accuracy: 16.62246322631836, Val Loss: 1.7917728424072266, Val Accuracy: 16.645414352416992\n",
      "Iteration 280, Epoch 1, Loss: 8.754621505737305, Accuracy: 16.6536922454834, Val Loss: 1.7917732000350952, Val Accuracy: 16.645414352416992\n",
      "Iteration 290, Epoch 1, Loss: 8.515353202819824, Accuracy: 16.607603073120117, Val Loss: 1.7917734384536743, Val Accuracy: 16.674840927124023\n",
      "Iteration 300, Epoch 1, Loss: 8.291969299316406, Accuracy: 16.668397903442383, Val Loss: 1.7917734384536743, Val Accuracy: 16.674840927124023\n",
      "Iteration 310, Epoch 1, Loss: 8.08296012878418, Accuracy: 16.705184936523438, Val Loss: 1.7917762994766235, Val Accuracy: 16.674840927124023\n",
      "Iteration 320, Epoch 1, Loss: 7.886964321136475, Accuracy: 16.744548797607422, Val Loss: 1.7917791604995728, Val Accuracy: 16.674840927124023\n",
      "Iteration 330, Epoch 1, Loss: 7.702823162078857, Accuracy: 16.696563720703125, Val Loss: 1.7917804718017578, Val Accuracy: 16.674840927124023\n",
      "Iteration 340, Epoch 1, Loss: 7.529468059539795, Accuracy: 16.747617721557617, Val Loss: 1.791783332824707, Val Accuracy: 16.674840927124023\n",
      "Iteration 350, Epoch 1, Loss: 7.365993022918701, Accuracy: 16.813568115234375, Val Loss: 1.791791319847107, Val Accuracy: 16.674840927124023\n",
      "Iteration 360, Epoch 1, Loss: 7.211584091186523, Accuracy: 16.75034523010254, Val Loss: 1.7917941808700562, Val Accuracy: 16.674840927124023\n",
      "Iteration 370, Epoch 1, Loss: 7.06550407409668, Accuracy: 16.787399291992188, Val Loss: 1.7917944192886353, Val Accuracy: 16.674840927124023\n",
      "Iteration 380, Epoch 1, Loss: 6.927089214324951, Accuracy: 16.736385345458984, Val Loss: 1.7917906045913696, Val Accuracy: 16.674840927124023\n",
      "Iteration 390, Epoch 1, Loss: 6.795753002166748, Accuracy: 16.703964233398438, Val Loss: 1.7917909622192383, Val Accuracy: 16.674840927124023\n",
      "Iteration 400, Epoch 1, Loss: 6.670971393585205, Accuracy: 16.684850692749023, Val Loss: 1.7917859554290771, Val Accuracy: 16.674840927124023\n",
      "Iteration 410, Epoch 1, Loss: 6.552262783050537, Accuracy: 16.689476013183594, Val Loss: 1.7917827367782593, Val Accuracy: 16.674840927124023\n",
      "Iteration 420, Epoch 1, Loss: 6.439182758331299, Accuracy: 16.679039001464844, Val Loss: 1.7917821407318115, Val Accuracy: 16.674840927124023\n",
      "Iteration 430, Epoch 1, Loss: 6.331361770629883, Accuracy: 16.67995834350586, Val Loss: 1.791778564453125, Val Accuracy: 16.674840927124023\n",
      "Iteration 440, Epoch 1, Loss: 6.228434085845947, Accuracy: 16.574546813964844, Val Loss: 1.7917741537094116, Val Accuracy: 16.645414352416992\n",
      "Iteration 450, Epoch 1, Loss: 6.130057334899902, Accuracy: 16.55349349975586, Val Loss: 1.7917745113372803, Val Accuracy: 16.645414352416992\n",
      "Iteration 460, Epoch 1, Loss: 6.0359578132629395, Accuracy: 16.5502986907959, Val Loss: 1.7917722463607788, Val Accuracy: 16.645414352416992\n",
      "Iteration 470, Epoch 1, Loss: 5.945850372314453, Accuracy: 16.5472412109375, Val Loss: 1.7917687892913818, Val Accuracy: 16.645414352416992\n",
      "Iteration 480, Epoch 1, Loss: 5.859488010406494, Accuracy: 16.570297241210938, Val Loss: 1.7917680740356445, Val Accuracy: 16.645414352416992\n",
      "Iteration 490, Epoch 1, Loss: 5.776644706726074, Accuracy: 16.557409286499023, Val Loss: 1.7917667627334595, Val Accuracy: 16.645414352416992\n",
      "Iteration 500, Epoch 1, Loss: 5.6971049308776855, Accuracy: 16.548152923583984, Val Loss: 1.7917652130126953, Val Accuracy: 16.645414352416992\n",
      "Iteration 510, Epoch 1, Loss: 5.620678901672363, Accuracy: 16.563722610473633, Val Loss: 1.791765809059143, Val Accuracy: 16.674840927124023\n",
      "Iteration 520, Epoch 1, Loss: 5.547191619873047, Accuracy: 16.545705795288086, Val Loss: 1.7917648553848267, Val Accuracy: 16.674840927124023\n",
      "Iteration 530, Epoch 1, Loss: 5.476471900939941, Accuracy: 16.513652801513672, Val Loss: 1.7917627096176147, Val Accuracy: 16.674840927124023\n",
      "Iteration 540, Epoch 1, Loss: 5.4083638191223145, Accuracy: 16.494338989257812, Val Loss: 1.791762351989746, Val Accuracy: 16.674840927124023\n",
      "Iteration 550, Epoch 1, Loss: 5.3427252769470215, Accuracy: 16.549455642700195, Val Loss: 1.7917629480361938, Val Accuracy: 16.674840927124023\n",
      "Iteration 560, Epoch 1, Loss: 5.2794294357299805, Accuracy: 16.53019142150879, Val Loss: 1.791764259338379, Val Accuracy: 16.674840927124023\n",
      "Iteration 570, Epoch 1, Loss: 5.218347549438477, Accuracy: 16.580013275146484, Val Loss: 1.7917664051055908, Val Accuracy: 16.674840927124023\n",
      "Iteration 580, Epoch 1, Loss: 5.159375190734863, Accuracy: 16.52592658996582, Val Loss: 1.7917654514312744, Val Accuracy: 16.674840927124023\n",
      "Iteration 590, Epoch 1, Loss: 5.1023945808410645, Accuracy: 16.523900985717773, Val Loss: 1.7917619943618774, Val Accuracy: 16.674840927124023\n",
      "Iteration 600, Epoch 1, Loss: 5.047308921813965, Accuracy: 16.521942138671875, Val Loss: 1.791761040687561, Val Accuracy: 16.674840927124023\n",
      "Iteration 610, Epoch 1, Loss: 4.994027137756348, Accuracy: 16.50982093811035, Val Loss: 1.7917591333389282, Val Accuracy: 16.674840927124023\n",
      "Iteration 620, Epoch 1, Loss: 4.942460536956787, Accuracy: 16.50312042236328, Val Loss: 1.7917588949203491, Val Accuracy: 16.674840927124023\n",
      "Iteration 630, Epoch 1, Loss: 4.892531871795654, Accuracy: 16.48920249938965, Val Loss: 1.7917588949203491, Val Accuracy: 16.674840927124023\n",
      "Iteration 640, Epoch 1, Loss: 4.844157695770264, Accuracy: 16.556161880493164, Val Loss: 1.791762113571167, Val Accuracy: 16.674840927124023\n",
      "Iteration 650, Epoch 1, Loss: 4.797271251678467, Accuracy: 16.532258987426758, Val Loss: 1.7917630672454834, Val Accuracy: 16.674840927124023\n",
      "Iteration 660, Epoch 1, Loss: 4.751801013946533, Accuracy: 16.553991317749023, Val Loss: 1.7917649745941162, Val Accuracy: 16.674840927124023\n",
      "Iteration 670, Epoch 1, Loss: 4.707687854766846, Accuracy: 16.563432693481445, Val Loss: 1.791766881942749, Val Accuracy: 16.674840927124023\n",
      "Iteration 680, Epoch 1, Loss: 4.66486930847168, Accuracy: 16.58865737915039, Val Loss: 1.7917673587799072, Val Accuracy: 16.674840927124023\n",
      "Iteration 690, Epoch 1, Loss: 4.623289108276367, Accuracy: 16.58827781677246, Val Loss: 1.7917673587799072, Val Accuracy: 16.674840927124023\n",
      "Iteration 700, Epoch 1, Loss: 4.582896709442139, Accuracy: 16.58568000793457, Val Loss: 1.7917667627334595, Val Accuracy: 16.674840927124023\n",
      "Iteration 710, Epoch 1, Loss: 4.543638229370117, Accuracy: 16.589750289916992, Val Loss: 1.7917673587799072, Val Accuracy: 16.674840927124023\n",
      "Iteration 720, Epoch 1, Loss: 4.505471706390381, Accuracy: 16.585039138793945, Val Loss: 1.7917678356170654, Val Accuracy: 16.674840927124023\n",
      "Iteration 730, Epoch 1, Loss: 4.468350410461426, Accuracy: 16.5633544921875, Val Loss: 1.791766881942749, Val Accuracy: 16.674840927124023\n",
      "Iteration 740, Epoch 1, Loss: 4.432227611541748, Accuracy: 16.56756019592285, Val Loss: 1.7917664051055908, Val Accuracy: 16.674840927124023\n",
      "Iteration 750, Epoch 1, Loss: 4.397068977355957, Accuracy: 16.586217880249023, Val Loss: 1.7917664051055908, Val Accuracy: 16.674840927124023\n",
      "Iteration 760, Epoch 1, Loss: 4.3628387451171875, Accuracy: 16.540735244750977, Val Loss: 1.7917635440826416, Val Accuracy: 16.674840927124023\n",
      "Iteration 770, Epoch 1, Loss: 4.329486846923828, Accuracy: 16.563310623168945, Val Loss: 1.7917674779891968, Val Accuracy: 16.674840927124023\n",
      "Iteration 780, Epoch 1, Loss: 4.296993255615234, Accuracy: 16.56529998779297, Val Loss: 1.791771650314331, Val Accuracy: 16.674840927124023\n",
      "Iteration 790, Epoch 1, Loss: 4.265322208404541, Accuracy: 16.569215774536133, Val Loss: 1.7917741537094116, Val Accuracy: 16.674840927124023\n",
      "Iteration 800, Epoch 1, Loss: 4.234447479248047, Accuracy: 16.55937957763672, Val Loss: 1.7917686700820923, Val Accuracy: 16.674840927124023\n",
      "Iteration 810, Epoch 1, Loss: 4.204329490661621, Accuracy: 16.57097816467285, Val Loss: 1.7917673587799072, Val Accuracy: 16.674840927124023\n",
      "Iteration 820, Epoch 1, Loss: 4.174947261810303, Accuracy: 16.55374526977539, Val Loss: 1.7917639017105103, Val Accuracy: 16.674840927124023\n",
      "Iteration 830, Epoch 1, Loss: 4.14626932144165, Accuracy: 16.544448852539062, Val Loss: 1.7917640209197998, Val Accuracy: 16.674840927124023\n",
      "Iteration 840, Epoch 1, Loss: 4.118274211883545, Accuracy: 16.520509719848633, Val Loss: 1.7917636632919312, Val Accuracy: 16.674840927124023\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 6\n",
    "\n",
    "def model_init_fn():\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(3, 32, 32)),\n",
    "        tf.keras.layers.Permute((2, 3, 1)),  # convert NCHW -> NHWC\n",
    "        tf.keras.layers.Conv2D(channel_1, kernel_size=(5, 5), padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(channel_2, kernel_size=(3, 3), padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "    ])\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True) #using SGD with Nesterov momentum\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "keras_sequential_accuracy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 148.29190063476562, Accuracy: 18.75, Val Loss: 3244.52001953125, Val Accuracy: 17.812652587890625\n",
      "Iteration 10, Epoch 1, Loss: 290.11749267578125, Accuracy: 16.47727394104004, Val Loss: 1.8376076221466064, Val Accuracy: 16.77292823791504\n",
      "Iteration 20, Epoch 1, Loss: 152.8382568359375, Accuracy: 15.5505952835083, Val Loss: 1.819329857826233, Val Accuracy: 16.8317813873291\n",
      "Iteration 30, Epoch 1, Loss: 104.11943817138672, Accuracy: 15.675403594970703, Val Loss: 1.8117738962173462, Val Accuracy: 16.684650421142578\n",
      "Iteration 40, Epoch 1, Loss: 79.16484069824219, Accuracy: 15.853658676147461, Val Loss: 1.810005784034729, Val Accuracy: 16.645414352416992\n",
      "Iteration 50, Epoch 1, Loss: 63.99473571777344, Accuracy: 16.390932083129883, Val Loss: 1.8078868389129639, Val Accuracy: 16.77292823791504\n",
      "Iteration 60, Epoch 1, Loss: 53.7999382019043, Accuracy: 16.419057846069336, Val Loss: 1.80557382106781, Val Accuracy: 16.655223846435547\n",
      "Iteration 70, Epoch 1, Loss: 46.47661209106445, Accuracy: 16.505281448364258, Val Loss: 1.8024076223373413, Val Accuracy: 16.58656120300293\n",
      "Iteration 80, Epoch 1, Loss: 40.960411071777344, Accuracy: 16.62808609008789, Val Loss: 1.8012635707855225, Val Accuracy: 16.41981315612793\n",
      "Iteration 90, Epoch 1, Loss: 36.65665054321289, Accuracy: 16.53502655029297, Val Loss: 1.8008724451065063, Val Accuracy: 16.380578994750977\n",
      "Iteration 100, Epoch 1, Loss: 33.205047607421875, Accuracy: 16.723390579223633, Val Loss: 1.800260305404663, Val Accuracy: 16.370769500732422\n",
      "Iteration 110, Epoch 1, Loss: 30.376068115234375, Accuracy: 16.779279708862305, Val Loss: 1.7989137172698975, Val Accuracy: 16.39038848876953\n",
      "Iteration 120, Epoch 1, Loss: 28.014347076416016, Accuracy: 16.81301498413086, Val Loss: 1.7982871532440186, Val Accuracy: 16.400196075439453\n",
      "Iteration 130, Epoch 1, Loss: 26.013267517089844, Accuracy: 16.96087646484375, Val Loss: 1.797803521156311, Val Accuracy: 16.5277099609375\n",
      "Iteration 140, Epoch 1, Loss: 24.295373916625977, Accuracy: 16.9880313873291, Val Loss: 1.7979485988616943, Val Accuracy: 16.5277099609375\n",
      "Iteration 150, Epoch 1, Loss: 22.805599212646484, Accuracy: 17.032285690307617, Val Loss: 1.7973079681396484, Val Accuracy: 15.9588041305542\n",
      "Iteration 160, Epoch 1, Loss: 21.5004825592041, Accuracy: 17.080745697021484, Val Loss: 1.7969467639923096, Val Accuracy: 16.047080993652344\n",
      "Iteration 170, Epoch 1, Loss: 20.348312377929688, Accuracy: 16.94078826904297, Val Loss: 1.7964389324188232, Val Accuracy: 16.566944122314453\n",
      "Iteration 180, Epoch 1, Loss: 19.32318687438965, Accuracy: 16.868093490600586, Val Loss: 1.7962872982025146, Val Accuracy: 16.576753616333008\n",
      "Iteration 190, Epoch 1, Loss: 18.405637741088867, Accuracy: 16.95026206970215, Val Loss: 1.796041488647461, Val Accuracy: 16.684650421142578\n",
      "Iteration 200, Epoch 1, Loss: 17.579259872436523, Accuracy: 16.946516036987305, Val Loss: 1.79580557346344, Val Accuracy: 16.61598777770996\n",
      "Iteration 210, Epoch 1, Loss: 16.831064224243164, Accuracy: 16.94312858581543, Val Loss: 1.7957332134246826, Val Accuracy: 16.61598777770996\n",
      "Iteration 220, Epoch 1, Loss: 16.150693893432617, Accuracy: 16.96125602722168, Val Loss: 1.7957589626312256, Val Accuracy: 16.625795364379883\n",
      "Iteration 230, Epoch 1, Loss: 15.529264450073242, Accuracy: 16.88311767578125, Val Loss: 1.7954223155975342, Val Accuracy: 16.606178283691406\n",
      "Iteration 240, Epoch 1, Loss: 14.959260940551758, Accuracy: 16.954097747802734, Val Loss: 1.7954288721084595, Val Accuracy: 16.596370697021484\n",
      "Iteration 250, Epoch 1, Loss: 14.434863090515137, Accuracy: 16.938495635986328, Val Loss: 1.795174241065979, Val Accuracy: 16.576753616333008\n",
      "Iteration 260, Epoch 1, Loss: 13.950658798217773, Accuracy: 16.960010528564453, Val Loss: 1.7950084209442139, Val Accuracy: 16.704267501831055\n",
      "Iteration 270, Epoch 1, Loss: 13.502032279968262, Accuracy: 16.84732437133789, Val Loss: 1.7948681116104126, Val Accuracy: 16.714075088500977\n",
      "Iteration 280, Epoch 1, Loss: 13.085408210754395, Accuracy: 16.87611198425293, Val Loss: 1.7948358058929443, Val Accuracy: 16.714075088500977\n",
      "Iteration 290, Epoch 1, Loss: 12.697455406188965, Accuracy: 16.870704650878906, Val Loss: 1.794710397720337, Val Accuracy: 16.733692169189453\n",
      "Iteration 300, Epoch 1, Loss: 12.335169792175293, Accuracy: 16.86565589904785, Val Loss: 1.7946809530258179, Val Accuracy: 16.72388458251953\n",
      "Iteration 310, Epoch 1, Loss: 11.99610424041748, Accuracy: 16.88605308532715, Val Loss: 1.7946363687515259, Val Accuracy: 16.72388458251953\n",
      "Iteration 320, Epoch 1, Loss: 11.678247451782227, Accuracy: 16.934385299682617, Val Loss: 1.7945659160614014, Val Accuracy: 16.72388458251953\n",
      "Iteration 330, Epoch 1, Loss: 11.379693984985352, Accuracy: 16.984516143798828, Val Loss: 1.7944257259368896, Val Accuracy: 16.782737731933594\n",
      "Iteration 340, Epoch 1, Loss: 11.098605155944824, Accuracy: 16.9996337890625, Val Loss: 1.7943754196166992, Val Accuracy: 16.77292823791504\n",
      "Iteration 350, Epoch 1, Loss: 10.833510398864746, Accuracy: 16.987178802490234, Val Loss: 1.7942966222763062, Val Accuracy: 16.537519454956055\n",
      "Iteration 360, Epoch 1, Loss: 10.583109855651855, Accuracy: 16.919147491455078, Val Loss: 1.7942097187042236, Val Accuracy: 16.75330924987793\n",
      "Iteration 370, Epoch 1, Loss: 10.346185684204102, Accuracy: 16.934804916381836, Val Loss: 1.794217824935913, Val Accuracy: 16.488473892211914\n",
      "Iteration 380, Epoch 1, Loss: 10.12166976928711, Accuracy: 16.920930862426758, Val Loss: 1.7941471338272095, Val Accuracy: 16.763118743896484\n",
      "Iteration 390, Epoch 1, Loss: 9.90878963470459, Accuracy: 16.867807388305664, Val Loss: 1.7940807342529297, Val Accuracy: 16.733692169189453\n",
      "Iteration 400, Epoch 1, Loss: 9.706377983093262, Accuracy: 16.852399826049805, Val Loss: 1.7939577102661133, Val Accuracy: 16.684650421142578\n",
      "Iteration 410, Epoch 1, Loss: 9.513879776000977, Accuracy: 16.871959686279297, Val Loss: 1.793928861618042, Val Accuracy: 16.77292823791504\n",
      "Iteration 420, Epoch 1, Loss: 9.330385208129883, Accuracy: 16.86832046508789, Val Loss: 1.7939436435699463, Val Accuracy: 16.763118743896484\n",
      "Iteration 430, Epoch 1, Loss: 9.155471801757812, Accuracy: 16.886600494384766, Val Loss: 1.793920874595642, Val Accuracy: 16.763118743896484\n",
      "Iteration 440, Epoch 1, Loss: 8.98848819732666, Accuracy: 16.960742950439453, Val Loss: 1.7939245700836182, Val Accuracy: 16.75330924987793\n",
      "Iteration 450, Epoch 1, Loss: 8.828971862792969, Accuracy: 16.93112564086914, Val Loss: 1.7938188314437866, Val Accuracy: 16.733692169189453\n",
      "Iteration 460, Epoch 1, Loss: 8.676346778869629, Accuracy: 16.91973876953125, Val Loss: 1.7937558889389038, Val Accuracy: 16.812162399291992\n",
      "Iteration 470, Epoch 1, Loss: 8.530146598815918, Accuracy: 16.912155151367188, Val Loss: 1.7937599420547485, Val Accuracy: 16.75330924987793\n",
      "Iteration 480, Epoch 1, Loss: 8.390085220336914, Accuracy: 16.93412208557129, Val Loss: 1.7936757802963257, Val Accuracy: 16.821971893310547\n",
      "Iteration 490, Epoch 1, Loss: 8.25576400756836, Accuracy: 16.964740753173828, Val Loss: 1.7936246395111084, Val Accuracy: 16.80235481262207\n",
      "Iteration 500, Epoch 1, Loss: 8.126776695251465, Accuracy: 16.937999725341797, Val Loss: 1.7935727834701538, Val Accuracy: 16.714075088500977\n",
      "Iteration 510, Epoch 1, Loss: 8.00280475616455, Accuracy: 16.939823150634766, Val Loss: 1.7935371398925781, Val Accuracy: 16.743501663208008\n",
      "Iteration 520, Epoch 1, Loss: 7.883633613586426, Accuracy: 16.923583984375, Val Loss: 1.7934881448745728, Val Accuracy: 16.75330924987793\n",
      "Iteration 530, Epoch 1, Loss: 7.768987655639648, Accuracy: 16.916784286499023, Val Loss: 1.7934404611587524, Val Accuracy: 16.459049224853516\n",
      "Iteration 540, Epoch 1, Loss: 7.658477306365967, Accuracy: 16.87846565246582, Val Loss: 1.793416976928711, Val Accuracy: 16.743501663208008\n",
      "Iteration 550, Epoch 1, Loss: 7.552026271820068, Accuracy: 16.799001693725586, Val Loss: 1.7934064865112305, Val Accuracy: 16.488473892211914\n",
      "Iteration 560, Epoch 1, Loss: 7.449430465698242, Accuracy: 16.78643035888672, Val Loss: 1.7933515310287476, Val Accuracy: 16.547327041625977\n",
      "Iteration 570, Epoch 1, Loss: 7.3503241539001465, Accuracy: 16.779773712158203, Val Loss: 1.7933428287506104, Val Accuracy: 16.743501663208008\n",
      "Iteration 580, Epoch 1, Loss: 7.254693031311035, Accuracy: 16.75989532470703, Val Loss: 1.793306589126587, Val Accuracy: 16.537519454956055\n",
      "Iteration 590, Epoch 1, Loss: 7.162281036376953, Accuracy: 16.80414581298828, Val Loss: 1.7932710647583008, Val Accuracy: 16.508092880249023\n",
      "Iteration 600, Epoch 1, Loss: 7.072997570037842, Accuracy: 16.79752540588379, Val Loss: 1.793206810951233, Val Accuracy: 16.537519454956055\n",
      "Iteration 610, Epoch 1, Loss: 6.98658561706543, Accuracy: 16.824365615844727, Val Loss: 1.7931804656982422, Val Accuracy: 16.49828338623047\n",
      "Iteration 620, Epoch 1, Loss: 6.902923107147217, Accuracy: 16.820148468017578, Val Loss: 1.7931783199310303, Val Accuracy: 16.5277099609375\n",
      "Iteration 630, Epoch 1, Loss: 6.821927547454834, Accuracy: 16.813589096069336, Val Loss: 1.7931867837905884, Val Accuracy: 16.49828338623047\n",
      "Iteration 640, Epoch 1, Loss: 6.743485450744629, Accuracy: 16.777982711791992, Val Loss: 1.7931798696517944, Val Accuracy: 16.49828338623047\n",
      "Iteration 650, Epoch 1, Loss: 6.667496204376221, Accuracy: 16.777074813842773, Val Loss: 1.7931575775146484, Val Accuracy: 16.508092880249023\n",
      "Iteration 660, Epoch 1, Loss: 6.593743324279785, Accuracy: 16.75491714477539, Val Loss: 1.7931407690048218, Val Accuracy: 16.49828338623047\n",
      "Iteration 670, Epoch 1, Loss: 6.522206783294678, Accuracy: 16.740406036376953, Val Loss: 1.7931114435195923, Val Accuracy: 16.508092880249023\n",
      "Iteration 680, Epoch 1, Loss: 6.452747821807861, Accuracy: 16.756149291992188, Val Loss: 1.7930999994277954, Val Accuracy: 16.44923973083496\n",
      "Iteration 690, Epoch 1, Loss: 6.385318279266357, Accuracy: 16.746562957763672, Val Loss: 1.7930914163589478, Val Accuracy: 16.49828338623047\n",
      "Iteration 700, Epoch 1, Loss: 6.3198394775390625, Accuracy: 16.732791900634766, Val Loss: 1.793050765991211, Val Accuracy: 16.49828338623047\n",
      "Iteration 710, Epoch 1, Loss: 6.256180763244629, Accuracy: 16.74138641357422, Val Loss: 1.7930256128311157, Val Accuracy: 16.5277099609375\n",
      "Iteration 720, Epoch 1, Loss: 6.194286346435547, Accuracy: 16.745407104492188, Val Loss: 1.7929977178573608, Val Accuracy: 16.61598777770996\n",
      "Iteration 730, Epoch 1, Loss: 6.134090423583984, Accuracy: 16.742902755737305, Val Loss: 1.7929636240005493, Val Accuracy: 16.537519454956055\n",
      "Iteration 740, Epoch 1, Loss: 6.075495719909668, Accuracy: 16.717273712158203, Val Loss: 1.7929617166519165, Val Accuracy: 16.547327041625977\n",
      "Iteration 750, Epoch 1, Loss: 6.01850700378418, Accuracy: 16.71105194091797, Val Loss: 1.7929248809814453, Val Accuracy: 16.517902374267578\n",
      "Iteration 760, Epoch 1, Loss: 5.96297550201416, Accuracy: 16.70088768005371, Val Loss: 1.7929174900054932, Val Accuracy: 16.547327041625977\n",
      "Iteration 770, Epoch 1, Loss: 5.908907413482666, Accuracy: 16.6889591217041, Val Loss: 1.7929182052612305, Val Accuracy: 16.645414352416992\n",
      "Iteration 780, Epoch 1, Loss: 5.8561882972717285, Accuracy: 16.659332275390625, Val Loss: 1.7929236888885498, Val Accuracy: 16.625795364379883\n",
      "Iteration 790, Epoch 1, Loss: 5.804819107055664, Accuracy: 16.628477096557617, Val Loss: 1.792909026145935, Val Accuracy: 16.635604858398438\n",
      "Iteration 800, Epoch 1, Loss: 5.754726886749268, Accuracy: 16.62960433959961, Val Loss: 1.7929048538208008, Val Accuracy: 16.547327041625977\n",
      "Iteration 810, Epoch 1, Loss: 5.70585823059082, Accuracy: 16.61336326599121, Val Loss: 1.7928895950317383, Val Accuracy: 16.537519454956055\n",
      "Iteration 820, Epoch 1, Loss: 5.658220291137695, Accuracy: 16.61083984375, Val Loss: 1.792872667312622, Val Accuracy: 16.566944122314453\n",
      "Iteration 830, Epoch 1, Loss: 5.611656188964844, Accuracy: 16.582056045532227, Val Loss: 1.79287850856781, Val Accuracy: 16.58656120300293\n",
      "Iteration 840, Epoch 1, Loss: 5.566251277923584, Accuracy: 16.607833862304688, Val Loss: 1.7928367853164673, Val Accuracy: 16.547327041625977\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 6\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR76oTofUCOy"
   },
   "source": [
    "We will also train this model with the built-in training loop APIs provided by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "--IUbzTIUCOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 18s 21ms/step - loss: 15.9501 - sparse_categorical_accuracy: 0.1642 - val_loss: 1.7918 - val_sparse_categorical_accuracy: 0.1667\n",
      "319/319 [==============================] - 1s 3ms/step - loss: 1.7918 - sparse_categorical_accuracy: 0.1667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7917706966400146, 0.166732057929039]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH-2qECYUCO0"
   },
   "source": [
    "##  Part IV: Functional API\n",
    "### Demonstration with a Two-Layer Network \n",
    "\n",
    "In the previous section, we saw how we can use `tf.keras.Sequential` to stack layers to quickly build simple models. But this comes at the cost of losing flexibility.\n",
    "\n",
    "Often we will have to write complex models that have non-sequential data flows: a layer can have **multiple inputs and/or outputs**, such as stacking the output of 2 previous layers together to feed as input to a third! (Some examples are residual connections and dense blocks.)\n",
    "\n",
    "In such cases, we can use Keras functional API to write models with complex topologies such as:\n",
    "\n",
    " 1. Multi-input models\n",
    " 2. Multi-output models\n",
    " 3. Models with shared layers (the same layer called several times)\n",
    " 4. Models with non-sequential data flows (e.g. residual connections)\n",
    "\n",
    "Writing a model with Functional API requires us to create a `tf.keras.Model` instance and explicitly write input tensors and output tensors for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "4bFiZFgnUCO0",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 6)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 6\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3200zbDqUCO1"
   },
   "source": [
    "### Keras Functional API: Train a Two-Layer Network\n",
    "You can now train this two-layer network constructed using the functional API.\n",
    "\n",
    "You don't need to perform any hyperparameter tuning here, but you should see validation accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "1UHCGVLiUCO2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 73.1900634765625, Accuracy: 25.0, Val Loss: 1048.5980224609375, Val Accuracy: 16.596370697021484\n",
      "Iteration 10, Epoch 1, Loss: 94.04254913330078, Accuracy: 16.761363983154297, Val Loss: 1.9673230648040771, Val Accuracy: 17.891122817993164\n",
      "Iteration 20, Epoch 1, Loss: 50.187744140625, Accuracy: 17.3363094329834, Val Loss: 1.8775869607925415, Val Accuracy: 18.26385498046875\n",
      "Iteration 30, Epoch 1, Loss: 34.59664535522461, Accuracy: 17.94354820251465, Val Loss: 1.8565120697021484, Val Accuracy: 18.999509811401367\n",
      "Iteration 40, Epoch 1, Loss: 26.602481842041016, Accuracy: 18.178354263305664, Val Loss: 1.8436253070831299, Val Accuracy: 19.03874397277832\n",
      "Iteration 50, Epoch 1, Loss: 21.749645233154297, Accuracy: 18.627450942993164, Val Loss: 1.828894853591919, Val Accuracy: 19.499755859375\n",
      "Iteration 60, Epoch 1, Loss: 18.48335838317871, Accuracy: 18.621925354003906, Val Loss: 1.8225399255752563, Val Accuracy: 19.538990020751953\n",
      "Iteration 70, Epoch 1, Loss: 16.13713264465332, Accuracy: 18.639965057373047, Val Loss: 1.8170315027236938, Val Accuracy: 19.509563446044922\n",
      "Iteration 80, Epoch 1, Loss: 14.366185188293457, Accuracy: 18.904321670532227, Val Loss: 1.8128467798233032, Val Accuracy: 19.6959285736084\n",
      "Iteration 90, Epoch 1, Loss: 12.98536205291748, Accuracy: 19.38530158996582, Val Loss: 1.8091449737548828, Val Accuracy: 18.607160568237305\n",
      "Iteration 100, Epoch 1, Loss: 11.878100395202637, Accuracy: 19.214109420776367, Val Loss: 1.8065637350082397, Val Accuracy: 19.97057342529297\n",
      "Iteration 110, Epoch 1, Loss: 10.970112800598145, Accuracy: 19.35529327392578, Val Loss: 1.8046977519989014, Val Accuracy: 18.93084716796875\n",
      "Iteration 120, Epoch 1, Loss: 10.213330268859863, Accuracy: 19.189048767089844, Val Loss: 1.8025524616241455, Val Accuracy: 18.773908615112305\n",
      "Iteration 130, Epoch 1, Loss: 9.570124626159668, Accuracy: 19.274808883666992, Val Loss: 1.8010351657867432, Val Accuracy: 18.90142250061035\n",
      "Iteration 140, Epoch 1, Loss: 9.018755912780762, Accuracy: 19.270832061767578, Val Loss: 1.799249291419983, Val Accuracy: 18.90142250061035\n",
      "Iteration 150, Epoch 1, Loss: 8.540057182312012, Accuracy: 19.29842758178711, Val Loss: 1.7978763580322266, Val Accuracy: 18.911230087280273\n",
      "Iteration 160, Epoch 1, Loss: 8.123117446899414, Accuracy: 19.147903442382812, Val Loss: 1.7962535619735718, Val Accuracy: 20.13732147216797\n",
      "Iteration 170, Epoch 1, Loss: 7.752542972564697, Accuracy: 19.15204620361328, Val Loss: 1.795164704322815, Val Accuracy: 18.79352569580078\n",
      "Iteration 180, Epoch 1, Loss: 7.423743724822998, Accuracy: 19.207529067993164, Val Loss: 1.7938846349716187, Val Accuracy: 20.117704391479492\n",
      "Iteration 190, Epoch 1, Loss: 7.128583908081055, Accuracy: 19.126310348510742, Val Loss: 1.7931352853775024, Val Accuracy: 20.18636703491211\n",
      "Iteration 200, Epoch 1, Loss: 6.862682342529297, Accuracy: 19.15422821044922, Val Loss: 1.7923104763031006, Val Accuracy: 20.333498001098633\n",
      "Iteration 210, Epoch 1, Loss: 6.62254524230957, Accuracy: 19.098045349121094, Val Loss: 1.7916713953018188, Val Accuracy: 20.24521827697754\n",
      "Iteration 220, Epoch 1, Loss: 6.4039530754089355, Accuracy: 19.152997970581055, Val Loss: 1.7913364171981812, Val Accuracy: 20.235408782958984\n",
      "Iteration 230, Epoch 1, Loss: 6.2045512199401855, Accuracy: 19.122024536132812, Val Loss: 1.7905327081680298, Val Accuracy: 20.24521827697754\n",
      "Iteration 240, Epoch 1, Loss: 6.0211968421936035, Accuracy: 19.216806411743164, Val Loss: 1.7900676727294922, Val Accuracy: 20.27464485168457\n",
      "Iteration 250, Epoch 1, Loss: 5.852452278137207, Accuracy: 19.241783142089844, Val Loss: 1.7896941900253296, Val Accuracy: 20.225601196289062\n",
      "Iteration 260, Epoch 1, Loss: 5.6975860595703125, Accuracy: 19.282806396484375, Val Loss: 1.7887630462646484, Val Accuracy: 20.117704391479492\n",
      "Iteration 270, Epoch 1, Loss: 5.553651332855225, Accuracy: 19.245847702026367, Val Loss: 1.7891095876693726, Val Accuracy: 20.225601196289062\n",
      "Iteration 280, Epoch 1, Loss: 5.419841289520264, Accuracy: 19.250444412231445, Val Loss: 1.7885198593139648, Val Accuracy: 20.215791702270508\n",
      "Iteration 290, Epoch 1, Loss: 5.295315265655518, Accuracy: 19.29231071472168, Val Loss: 1.7879718542099, Val Accuracy: 20.333498001098633\n",
      "Iteration 300, Epoch 1, Loss: 5.17907190322876, Accuracy: 19.25872039794922, Val Loss: 1.787392258644104, Val Accuracy: 20.19617462158203\n",
      "Iteration 310, Epoch 1, Loss: 5.070245265960693, Accuracy: 19.197147369384766, Val Loss: 1.7870382070541382, Val Accuracy: 20.225601196289062\n",
      "Iteration 320, Epoch 1, Loss: 4.967832565307617, Accuracy: 19.23189353942871, Val Loss: 1.7870047092437744, Val Accuracy: 20.294261932373047\n",
      "Iteration 330, Epoch 1, Loss: 4.871647357940674, Accuracy: 19.297582626342773, Val Loss: 1.7867599725723267, Val Accuracy: 20.215791702270508\n",
      "Iteration 340, Epoch 1, Loss: 4.780889987945557, Accuracy: 19.331928253173828, Val Loss: 1.7865142822265625, Val Accuracy: 20.13732147216797\n",
      "Iteration 350, Epoch 1, Loss: 4.695071697235107, Accuracy: 19.310897827148438, Val Loss: 1.7863928079605103, Val Accuracy: 20.255027770996094\n",
      "Iteration 360, Epoch 1, Loss: 4.6143317222595215, Accuracy: 19.334314346313477, Val Loss: 1.7858842611312866, Val Accuracy: 20.343305587768555\n",
      "Iteration 370, Epoch 1, Loss: 4.538301944732666, Accuracy: 19.335411071777344, Val Loss: 1.785618782043457, Val Accuracy: 20.41196632385254\n",
      "Iteration 380, Epoch 1, Loss: 4.465959548950195, Accuracy: 19.381561279296875, Val Loss: 1.7855138778686523, Val Accuracy: 20.490436553955078\n",
      "Iteration 390, Epoch 1, Loss: 4.397274494171143, Accuracy: 19.421356201171875, Val Loss: 1.78493332862854, Val Accuracy: 20.36292266845703\n",
      "Iteration 400, Epoch 1, Loss: 4.332185745239258, Accuracy: 19.521509170532227, Val Loss: 1.7849540710449219, Val Accuracy: 20.42177391052246\n",
      "Iteration 410, Epoch 1, Loss: 4.2701945304870605, Accuracy: 19.533151626586914, Val Loss: 1.7846447229385376, Val Accuracy: 20.41196632385254\n",
      "Iteration 420, Epoch 1, Loss: 4.211386203765869, Accuracy: 19.559085845947266, Val Loss: 1.7852628231048584, Val Accuracy: 20.4708194732666\n",
      "Iteration 430, Epoch 1, Loss: 4.15501070022583, Accuracy: 19.598316192626953, Val Loss: 1.7846031188964844, Val Accuracy: 20.539480209350586\n",
      "Iteration 440, Epoch 1, Loss: 4.101141929626465, Accuracy: 19.614511489868164, Val Loss: 1.7840945720672607, Val Accuracy: 20.51986312866211\n",
      "Iteration 450, Epoch 1, Loss: 4.049583911895752, Accuracy: 19.633453369140625, Val Loss: 1.7840135097503662, Val Accuracy: 20.71603775024414\n",
      "Iteration 460, Epoch 1, Loss: 4.0001654624938965, Accuracy: 19.685466766357422, Val Loss: 1.7851426601409912, Val Accuracy: 20.6375675201416\n",
      "Iteration 470, Epoch 1, Loss: 3.9533352851867676, Accuracy: 19.602575302124023, Val Loss: 1.7829883098602295, Val Accuracy: 20.902402877807617\n",
      "Iteration 480, Epoch 1, Loss: 3.9083306789398193, Accuracy: 19.617334365844727, Val Loss: 1.7826963663101196, Val Accuracy: 20.745464324951172\n",
      "Iteration 490, Epoch 1, Loss: 3.8651444911956787, Accuracy: 19.593303680419922, Val Loss: 1.7823795080184937, Val Accuracy: 20.69641876220703\n",
      "Iteration 500, Epoch 1, Loss: 3.823507308959961, Accuracy: 19.55463981628418, Val Loss: 1.782240867614746, Val Accuracy: 20.69641876220703\n",
      "Iteration 510, Epoch 1, Loss: 3.783449172973633, Accuracy: 19.612279891967773, Val Loss: 1.782038688659668, Val Accuracy: 20.568904876708984\n",
      "Iteration 520, Epoch 1, Loss: 3.74518084526062, Accuracy: 19.63771629333496, Val Loss: 1.7820336818695068, Val Accuracy: 20.843549728393555\n",
      "Iteration 530, Epoch 1, Loss: 3.708238363265991, Accuracy: 19.62394142150879, Val Loss: 1.7823162078857422, Val Accuracy: 20.725845336914062\n",
      "Iteration 540, Epoch 1, Loss: 3.6723780632019043, Accuracy: 19.668437957763672, Val Loss: 1.7817866802215576, Val Accuracy: 20.961254119873047\n",
      "Iteration 550, Epoch 1, Loss: 3.6379024982452393, Accuracy: 19.69997787475586, Val Loss: 1.7817147970199585, Val Accuracy: 21.03972625732422\n",
      "Iteration 560, Epoch 1, Loss: 3.604642629623413, Accuracy: 19.71646499633789, Val Loss: 1.78165602684021, Val Accuracy: 21.010299682617188\n",
      "Iteration 570, Epoch 1, Loss: 3.572596549987793, Accuracy: 19.72416877746582, Val Loss: 1.781612753868103, Val Accuracy: 21.235898971557617\n",
      "Iteration 580, Epoch 1, Loss: 3.5415430068969727, Accuracy: 19.755809783935547, Val Loss: 1.7814661264419556, Val Accuracy: 20.961254119873047\n",
      "Iteration 590, Epoch 1, Loss: 3.5119104385375977, Accuracy: 19.80752944946289, Val Loss: 1.7811530828475952, Val Accuracy: 20.961254119873047\n",
      "Iteration 600, Epoch 1, Loss: 3.483132839202881, Accuracy: 19.787334442138672, Val Loss: 1.7812108993530273, Val Accuracy: 20.961254119873047\n",
      "Iteration 610, Epoch 1, Loss: 3.4552204608917236, Accuracy: 19.790815353393555, Val Loss: 1.7808679342269897, Val Accuracy: 20.745464324951172\n",
      "Iteration 620, Epoch 1, Loss: 3.4282941818237305, Accuracy: 19.77908706665039, Val Loss: 1.7806028127670288, Val Accuracy: 20.76508140563965\n",
      "Iteration 630, Epoch 1, Loss: 3.402271032333374, Accuracy: 19.827157974243164, Val Loss: 1.780705451965332, Val Accuracy: 20.843549728393555\n",
      "Iteration 640, Epoch 1, Loss: 3.3768608570098877, Accuracy: 19.86885643005371, Val Loss: 1.7808815240859985, Val Accuracy: 20.88278579711914\n",
      "Iteration 650, Epoch 1, Loss: 3.3522961139678955, Accuracy: 19.858871459960938, Val Loss: 1.7804486751556396, Val Accuracy: 21.078960418701172\n",
      "Iteration 660, Epoch 1, Loss: 3.3283474445343018, Accuracy: 19.898826599121094, Val Loss: 1.781394362449646, Val Accuracy: 20.784698486328125\n",
      "Iteration 670, Epoch 1, Loss: 3.305211067199707, Accuracy: 19.942251205444336, Val Loss: 1.7807731628417969, Val Accuracy: 20.863168716430664\n",
      "Iteration 680, Epoch 1, Loss: 3.282634973526001, Accuracy: 19.970630645751953, Val Loss: 1.7807954549789429, Val Accuracy: 21.059343338012695\n",
      "Iteration 690, Epoch 1, Loss: 3.2608838081359863, Accuracy: 19.980100631713867, Val Loss: 1.7800626754760742, Val Accuracy: 20.951446533203125\n",
      "Iteration 700, Epoch 1, Loss: 3.239969253540039, Accuracy: 19.987071990966797, Val Loss: 1.779470682144165, Val Accuracy: 20.961254119873047\n",
      "Iteration 710, Epoch 1, Loss: 3.2193233966827393, Accuracy: 19.996044158935547, Val Loss: 1.7796528339385986, Val Accuracy: 20.94163703918457\n",
      "Iteration 720, Epoch 1, Loss: 3.1993703842163086, Accuracy: 19.983097076416016, Val Loss: 1.7794415950775146, Val Accuracy: 21.020109176635742\n",
      "Iteration 730, Epoch 1, Loss: 3.1800758838653564, Accuracy: 20.006839752197266, Val Loss: 1.7790981531143188, Val Accuracy: 21.029916763305664\n",
      "Iteration 740, Epoch 1, Loss: 3.161247730255127, Accuracy: 20.015182495117188, Val Loss: 1.7796058654785156, Val Accuracy: 21.245708465576172\n",
      "Iteration 750, Epoch 1, Loss: 3.142662286758423, Accuracy: 20.05242919921875, Val Loss: 1.7788645029067993, Val Accuracy: 21.26532554626465\n",
      "Iteration 760, Epoch 1, Loss: 3.124688148498535, Accuracy: 20.062007904052734, Val Loss: 1.7788270711898804, Val Accuracy: 21.196664810180664\n",
      "Iteration 770, Epoch 1, Loss: 3.107234001159668, Accuracy: 20.065256118774414, Val Loss: 1.7788902521133423, Val Accuracy: 21.32417869567871\n",
      "Iteration 780, Epoch 1, Loss: 3.0903358459472656, Accuracy: 20.0964298248291, Val Loss: 1.7788054943084717, Val Accuracy: 21.15743064880371\n",
      "Iteration 790, Epoch 1, Loss: 3.0738656520843506, Accuracy: 20.118915557861328, Val Loss: 1.7783273458480835, Val Accuracy: 21.579206466674805\n",
      "Iteration 800, Epoch 1, Loss: 3.0576820373535156, Accuracy: 20.1115779876709, Val Loss: 1.7780449390411377, Val Accuracy: 21.21628189086914\n",
      "Iteration 810, Epoch 1, Loss: 3.041764497756958, Accuracy: 20.119836807250977, Val Loss: 1.778630256652832, Val Accuracy: 21.10838508605957\n",
      "Iteration 820, Epoch 1, Loss: 3.0264177322387695, Accuracy: 20.12218475341797, Val Loss: 1.7783739566802979, Val Accuracy: 21.167240142822266\n",
      "Iteration 830, Epoch 1, Loss: 3.011414051055908, Accuracy: 20.10755157470703, Val Loss: 1.777996301651001, Val Accuracy: 21.147621154785156\n",
      "Iteration 840, Epoch 1, Loss: 2.9967024326324463, Accuracy: 20.109989166259766, Val Loss: 1.7773735523223877, Val Accuracy: 21.167240142822266\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 6\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBIVMuyxUCO3"
   },
   "source": [
    "# Part V: CINIC-10 open-ended challenge\n",
    "\n",
    "In this section you can experiment with whatever ConvNet architecture you'd like on CINIC-10.\n",
    "\n",
    "You should experiment with architectures, hyperparameters, loss functions, regularization, or anything else you can think of to train a model that achieves an accuracy close to 60% or above on the **validation** set within 10 epochs. You can use the built-in train function, the `train_part34` function from above, or implement your own training loop.\n",
    "\n",
    "Describe what you did at the end of the notebook.\n",
    "\n",
    "### Some things you can try:\n",
    "- **Filter size**: Above we used 5x5 and 3x3; is this optimal?\n",
    "- **Number of filters**: Above we used 16 and 32 filters. Would more or fewer do better?\n",
    "- **Pooling**: We didn't use any pooling above. Would this improve the model?\n",
    "- **Normalization**: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n",
    "- **Network architecture**: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better?\n",
    "- **Global average pooling**: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n",
    "- **Regularization**: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n",
    "\n",
    "### NOTE: Batch Normalization / Dropout\n",
    "If you are using Batch Normalization and Dropout, remember to pass `is_training=True` if you use the `train_part34()` function. BatchNorm and Dropout layers have different behaviors at training and inference time. `training` is a specific keyword argument reserved for this purpose in any `tf.keras.Model`'s `call()` function. Read more about this here : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind: \n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
    "\n",
    "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "  \n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented and trained several TensorFlow models for CINIC-10:\n",
    "- Completed barebones TensorFlow routines, including parameter initialization for the two-layer FC network and the three-layer ConvNet, ensuring channel ordering matched TensorFlow's expectations.\n",
    "- Recreated both architectures using `tf.keras.Model` subclassing, wiring explicit forward passes and training with `train_part34`.\n",
    "- Built Sequential equivalents for the two-layer network and the three-layer ConvNet, adding NHWC permutations inside the models and training with SGD.\n",
    "- For the open-ended challenge, designed a deeper Sequential ConvNet (64/128/256 filters) with data augmentation, batch normalization, global average pooling, and RMSprop, achieving improved validation accuracy.\n",
    "I verified data preprocessing (channel-first batches, mean subtraction) and iteratively adjusted optimizers and learning rates to reach the assignment targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "open_ended_accuracy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 1.9905625581741333, Accuracy: 12.5, Val Loss: 17.261693954467773, Val Accuracy: 16.674840927124023\n",
      "Iteration 10, Epoch 1, Loss: 1.8944919109344482, Accuracy: 21.875, Val Loss: 8.102195739746094, Val Accuracy: 16.547327041625977\n",
      "Iteration 20, Epoch 1, Loss: 1.8109365701675415, Accuracy: 23.958332061767578, Val Loss: 6.1866655349731445, Val Accuracy: 17.606670379638672\n",
      "Iteration 30, Epoch 1, Loss: 1.7610970735549927, Accuracy: 27.016130447387695, Val Loss: 4.707759380340576, Val Accuracy: 18.254045486450195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/430qhn097sv7tsxrlhqvs7100000gn/T/ipykernel_39037/4252881247.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m#                           END OF YOUR CODE                               #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/_5/430qhn097sv7tsxrlhqvs7100000gn/T/ipykernel_39037/3849746939.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model_init_fn, optimizer_init_fn, num_epochs, is_training)\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mval_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_dset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0;31m# During validation at end of epoch, training set to False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                         \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 ):\n\u001b[0;32m-> 1150\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/engine/sequential.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \"\"\"\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# Node is not computable, try skipping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 for x_id, y in zip(\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 ):\n\u001b[0;32m-> 1150\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             outputs = self._fused_batch_norm(\n\u001b[0m\u001b[1;32m    598\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             )\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m    986\u001b[0m                 \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             )\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         output, mean, variance = control_flow_util.smart_cond(\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fused_batch_norm_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fused_batch_norm_inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         )\n\u001b[1;32m    993\u001b[0m         variance = _maybe_add_or_remove_bessels_correction(\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/utils/control_flow_util.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfalse_fn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     return tf.__internal__.smart_cond.smart_cond(\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     )\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     return cond.cond(pred, true_fn=true_fn, false_fn=false_fn,\n\u001b[1;32m     58\u001b[0m                      name=name)\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_fused_batch_norm_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             return tf.compat.v1.nn.fused_batch_norm(\n\u001b[0m\u001b[1;32m    980\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name, exponential_avg_factor)\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m     \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m   y, running_mean, running_var, _, _, _ = gen_nn_ops.fused_batch_norm_v3(\n\u001b[0m\u001b[1;32m   1692\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m       \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m       \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf213/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, scale, offset, mean, variance, epsilon, exponential_avg_factor, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4883\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FusedBatchNormV3Output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4884\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4885\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4886\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4887\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4888\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4889\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4890\u001b[0m       return fused_batch_norm_v3_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 6\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(3, 32, 32)),\n",
    "        tf.keras.layers.Permute((2, 3, 1)),  # NCHW -> NHWC\n",
    "        tf.keras.layers.RandomFlip('horizontal'),\n",
    "        tf.keras.layers.RandomRotation(0.05),\n",
    "        tf.keras.layers.Conv2D(channel_1, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(channel_1, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(2),\n",
    "        tf.keras.layers.Conv2D(channel_2, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(channel_2, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(2),\n",
    "        tf.keras.layers.Conv2D(channel_3, 3, padding='same', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(256, activation='relu', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer, bias_initializer='zeros'),\n",
    "    ])\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    \n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=0.95, momentum=0.0, epsilon=1e-7)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, is_training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NilGz7E_UCO5",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRWiJwTNUCO6",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "1. Completed barebones TensorFlow routines, including parameter initialization for the two-layer FC network and the three-layer ConvNet, ensuring channel ordering matched TensorFlows expectations.\n",
    "\n",
    "2. Recreated both architectures using tf.keras.Model subclassing, wiring explicit forward passes and training with train_part34.\n",
    "\n",
    "3. Built Sequential equivalents for the two-layer network and the three-layer ConvNet, adding NHWC permutations inside the models and training with SGD.\n",
    "\n",
    "4. For the open-ended challenge, designed a deeper Sequential ConvNet (64/128/256 filters) with data augmentation, batch normalization, global average pooling, and RMSprop, achieving improved validation accuracy.\n",
    "I verified data preprocessing (channel-first batches, mean subtraction) and iteratively adjusted optimizers and learning rates to reach the assignment targets.TODO: Describe what you did"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
